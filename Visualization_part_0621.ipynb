{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Visualization_2021_v3 셀 수행\n",
    "\n",
    "## MTP / 변경 전\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.dates import DateFormatter\n",
    "from matplotlib import cm\n",
    "import os\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "import time\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "\n",
    "\n",
    "#### 공용 변수\n",
    "path = None\n",
    "target_path = None\n",
    "\n",
    "# System 정보\n",
    "system_info = ['TIMESTAMP', 'Cluster', 'NodeId', 'Generation', 'HwSkuId',\n",
    "               'DriveProductId', 'DriveSerialNumber', 'FirmwareRevision']\n",
    "\n",
    "# SMART 항목\n",
    "item_smart = ['CritWarning', 'Temperature', 'AvailableSpare', 'AvailSpareThreshold',\n",
    "              'PercentageUsed', 'DataUnitsRead', 'DataUnitsWritten',\n",
    "              'HostReadCommands', 'HostWriteCommands', 'ControllerBusyTime',\n",
    "              'PowerCycles', 'PowerOnHours', 'UnsafeShutdowns', 'MediaErrors',\n",
    "              'NumErrInfoLogEntries', 'WarnCompositeTempTime',\n",
    "              'CritCompositeTempTime', 'TempSensor1', 'TempSensor2', 'TempSensor3',\n",
    "              'TempSensor4', 'TempSensor5', 'TempSensor6', 'TempSensor7',\n",
    "              'TempSensor8']\n",
    "\n",
    "# Extended SMART 항목 전부 표기 - KDE 미수행 항목들은 뒤쪽에 표기\n",
    "item_ext_smart = ['Media_Units_Written', 'ECC_Iterations', 'Wear_Range_Delta',\n",
    "                  'Unaligned_IO', 'Mapped_LBAs', 'Program_Fail_Count', 'Erase_Fail_Count',\n",
    "                  'Capacitor_Health', 'Supported_Features', 'Power_Consumption',\n",
    "                  'Temperature_Throttling']\n",
    "\n",
    "\n",
    "def millions(x,pos):\n",
    "    return '%1.0fM' % (x*1e-6)\n",
    "\n",
    "def kilo(x,pos):\n",
    "    return '%1.0fK' % (x*1e-3)\n",
    "\n",
    " #DriveSerialNumber 하나에 대해 여러 SSDUID를 가지는 경우 가장 값이 많이 나타나는 SSDUID 기준으로 필터링하는 함수\n",
    "# 2021.08.03 수정 - 대문자 변환\n",
    "def get_df_1ssduid(df):\n",
    "    df['SSDUID'] = df['SSDUID'].str.upper()\n",
    "    vc = df['SSDUID'].value_counts()\n",
    "    return df[df['SSDUID']==vc.index[0]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def Visualization_main(target_product):\n",
    "    global path, target_path, product, system_info, item_smart, item_ext_smart, cols, crit_warn_info, item_list\n",
    "\n",
    "    product = target_product\n",
    "\n",
    "    # 결과 파일 (Abnormal Data / Figure) 저장 경로\n",
    "    if os.path.exists(target_path+'/0_Abnormal_Data') == False:\n",
    "        os.mkdir(target_path+'/0_Abnormal_Data')\n",
    "    if os.path.exists(target_path+'/1_Figure') == False:\n",
    "        os.mkdir(target_path+'/1_Figure')\n",
    "\n",
    "    register_matplotlib_converters()\n",
    "\n",
    "\n",
    "    file_list = os.listdir(path)\n",
    "    file_list.sort()\n",
    "\n",
    "    if product == 'PM963':  # PM963: Temp Sensor 2까지 있음\n",
    "        item_list = item_smart[1:3] + item_smart[4:19]\n",
    "    elif product in ['PM983', 'PM1725b']:  # PM983 & PM1725b: Temp Sensor 3까지 있음\n",
    "        item_list = item_smart[1:3] + item_smart[4:20]\n",
    "    elif product == 'PM953':  # PM953: NVMe 1.1 spec - No. of Err Info Log Entries까지\n",
    "        item_list = item_smart[1:3] + item_smart[4:15]\n",
    "\n",
    "        # Extended SMART 항목 포함 (제품별 차이)\n",
    "    if product in ['PM963', 'PM983']:\n",
    "        item_list = item_list + item_ext_smart[:7]\n",
    "    elif product == 'PM953':\n",
    "        item_list = item_list + [item_ext_smart[0]] + item_ext_smart[2:5]\n",
    "\n",
    "\n",
    "    # Telemetry 항목값의 Type 지정 : 필요 시 설정 (기본 None)\n",
    "    types_dict = None\n",
    "\n",
    "    # 데이터 전체 column 로드 시 category화할 column들\n",
    "    cols_category = system_info[1:6] + [system_info[7]]\n",
    "    #crit_warn_info = pd.read_csv('Anomaly_Rulebase_{}.csv'.format(product), index_col=0)\n",
    "    \"\"\"\n",
    "    220621 수정 kdy\n",
    "    \"\"\"\n",
    "    #crit_warn_info = sqlContext.read.format('com.databricks.spark.csv')\\\n",
    "    #       .options(header='true', inferSchema='true')\\\n",
    "    #      .load('Anomaly_Rulebase_{}.csv'.format(product))\n",
    "\n",
    "    crit_warn_info = spark.read.format('com.databricks.spark.csv')\\\n",
    "    .options(header='true', inferSchema='true').load('/Users/AitheNutrigene/Desktop/Azure_pyspark/Anomaly_Rulebase_{}.csv'.format('product'))#20220627 kdy\n",
    "    \n",
    "    colormap1 = cm.viridis\n",
    "\n",
    "\n",
    "\n",
    "    ###### Visualization (Long / Short)\n",
    "    # 최신 File로부터 모든 항목의 Critical/Warning SSD의 SN 가져오기\n",
    "    \n",
    "\n",
    "    #crit_list = pd.DataFrame().index\n",
    "    #warn_list = pd.DataFrame().index\n",
    "\n",
    "    crit_list = spark.sparkContext.emptyRDD() #Empty DataFrame 생성 #20220627 kdy\n",
    "    warn_list = spark.sparkContext.emptyRDD() #20220627 kdy\n",
    "\n",
    "\n",
    "    for selected_column in item_list:\n",
    "        tmp = os.listdir(target_path + '/' + selected_column + '/')\n",
    "        tmp.sort()\n",
    "        all_file_list = pd.DataFrame(tmp)\n",
    "        \"\"\"\n",
    "        crit_file_list = list(all_file_list[(all_file_list.loc[:, 0].str.endswith(\".csv\") == True) &\n",
    "                                            (all_file_list.loc[:, 0].str.startswith(\"Crit\") == True)].loc[:, 0])\n",
    "        warn_file_list = list(all_file_list[(all_file_list.loc[:, 0].str.endswith(\".csv\") == True) &\n",
    "                                            (all_file_list.loc[:, 0].str.startswith(\"Warn\") == True)].loc[:, 0])\n",
    "        \n",
    "        crit = pd.read_csv(target_path + '/' + selected_column + '/' + crit_file_list[-1], index_col=1).index\n",
    "        crit_list = crit_list.union(crit)\n",
    "        warn = pd.read_csv(target_path + '/' + selected_column + '/' + warn_file_list[-1], index_col=1).index\n",
    "        warn_list = warn_list.union(warn)\"\"\"\n",
    "        \n",
    "        crit = spark.read.format('com.databricks.spark.csv')\\\n",
    "            .options(header='true', inferSchema='true')\\\n",
    "            .load(target_path + '/' + selected_column +'/'+'Crit_' + file)#20220627 kdy\n",
    "\n",
    "    ssd_list = crit_list.union(warn_list)\n",
    "\n",
    "    # 모든 Critical/Warning drive들의 long-term data DataFrame\n",
    "    start = time.time()\n",
    "    data_total = pd.DataFrame()\n",
    "\n",
    "\n",
    "    df_pathList = spark.read.format(\"csv\").load(pathList)\n",
    "    file_list = df_pathList.inputFiles()  \n",
    "\n",
    "    for file in file_list:\n",
    "        print('Loading filename {}: '.format(file) + time.strftime('%Y-%m-%d %H:%M:%S', time.localtime()))\n",
    "        tmp_data = pd.DataFrame()\n",
    "        for chunk in pd.read_csv(path + '/' + file, chunksize=1000000, dtype=types_dict):\n",
    "            chunk = chunk[chunk['DriveSerialNumber'].isin(ssd_list)]\n",
    "            chunk['date'] = chunk.TIMESTAMP.str[:10]\n",
    "            tmp_data = pd.concat([tmp_data, chunk])\n",
    "        # Category화를 통한 소요 메모리 절약\n",
    "        for col in cols_category:\n",
    "            tmp_data[col] = tmp_data[col].astype('category')\n",
    "        tmp_data['date'] = tmp_data['date'].astype('category')\n",
    "        tmp_data = tmp_data.sort_values(by=['DriveSerialNumber', 'TIMESTAMP'])  # Sort by: DriveSerialNumber, TIMESTAMP\n",
    "\n",
    "        data_total = pd.concat([data_total, tmp_data])\n",
    "\n",
    "    data_total = data_total.sort_values(by=['DriveSerialNumber', 'TIMESTAMP'])  # Sort by: DriveSerialNumber, TIMESTAMP\n",
    "    end = time.time()\n",
    "    print(f'Elapsed time : {(end-start):.3f} seconds')\n",
    "\n",
    "    # groupby 사용해서 각 DriveSerialNumber에 대해 유일한 SSDUID만 남기기\n",
    "    start = time.time()\n",
    "    grouped = data_total.groupby(by='DriveSerialNumber')\n",
    "    data_long_fixed = grouped.apply(get_df_1ssduid)\n",
    "    data_long_fixed = data_long_fixed.reset_index(drop=True)\n",
    "    end = time.time()\n",
    "    print(f'Elapsed time : {(end-start):.3f} seconds')\n",
    "    print('Total no. of rows in crit/warn drive data : {}'.format(len(data_long_fixed)))\n",
    "\n",
    "    # 마지막 주 데이터의 날짜 중 가장 오래된 값 - short data의 기준 날짜\n",
    "    short_datetime = tmp_data['date'].astype('datetime64[ns]').min()\n",
    "\n",
    "    # Generate short-term Crit/Warn data\n",
    "    data_long_fixed['date'] = pd.to_datetime(data_long_fixed['date'])\n",
    "    data_short = data_long_fixed[data_long_fixed['date'] >= short_datetime]\n",
    "\n",
    "    for selected_column in item_list:\n",
    "        print('Processing {}: '.format(selected_column) +\n",
    "              time.strftime('%Y-%m-%d %H:%M:%S', time.localtime()))\n",
    "        start = time.time()\n",
    "\n",
    "        ## Critical / Warning SN 정보 및 데이터 분포 정보 불러오기\n",
    "        tmp = os.listdir(target_path +'/' + selected_column + '/')\n",
    "        tmp.sort()\n",
    "        tmp_info = crit_warn_info.loc[selected_column.lower()]\n",
    "        all_file_list = pd.DataFrame(tmp)\n",
    "        value_file_list = list(all_file_list[(all_file_list.loc[:, 0].str.endswith(\".csv\") == True) &\n",
    "                                             (all_file_list.loc[:, 0].str.startswith(\"Value\") == True)].loc[:, 0])\n",
    "        weight_file_list = list(all_file_list[(all_file_list.loc[:, 0].str.endswith(\".csv\") == True) &\n",
    "                                              (all_file_list.loc[:, 0].str.startswith(\"Weight\") == True)].loc[:, 0])\n",
    "        crit_file_list = list(all_file_list[(all_file_list.loc[:, 0].str.endswith(\".csv\") == True) &\n",
    "                                            (all_file_list.loc[:, 0].str.startswith(\"Crit\") == True)].loc[:, 0])\n",
    "        warn_file_list = list(all_file_list[(all_file_list.loc[:, 0].str.endswith(\".csv\") == True) &\n",
    "                                            (all_file_list.loc[:, 0].str.startswith(\"Warn\") == True)].loc[:, 0])\n",
    "        boundary_file_list = list(all_file_list[(all_file_list.loc[:, 0].str.endswith(\".csv\") == True) &\n",
    "                                                (all_file_list.loc[:, 0].str.startswith(\"Boundary\") == True)].loc[:, 0])\n",
    "\n",
    "        # 데이터 구간 정보\n",
    "        for file in value_file_list:\n",
    "            value_temp = pd.read_csv(target_path + '/' + selected_column + '/' + file, index_col=0)\n",
    "            if file == value_file_list[0]:\n",
    "                value_tot = value_temp\n",
    "            else:\n",
    "                value_tot = pd.concat((value_tot, value_temp), axis=0)\n",
    "\n",
    "        value_tot = np.array(value_tot.reset_index().iloc[:, 1:])\n",
    "\n",
    "        value_short = value_temp\n",
    "        value_short = np.array(value_short.reset_index().iloc[:, 1:])\n",
    "\n",
    "        # 데이터 분포 정보\n",
    "        for file in weight_file_list:\n",
    "            weight_temp = pd.read_csv(target_path +'/' + selected_column + '/' + file, index_col=0)\n",
    "            if file == weight_file_list[0]:\n",
    "                weight_tot = weight_temp\n",
    "            else:\n",
    "                weight_tot = pd.concat((weight_tot, weight_temp), axis=0)\n",
    "\n",
    "        weight_tot = np.array(weight_tot.reset_index().iloc[:, 1:])\n",
    "\n",
    "        weight_short = weight_temp\n",
    "        weight_short = np.array(weight_short.reset_index().iloc[:, 1:])\n",
    "\n",
    "        # Critical / Warning 정보: 최신 데이터 사용\n",
    "        crit_file = crit_file_list[-1]\n",
    "        crit_tot = pd.read_csv(target_path +'/' + selected_column + '/' + crit_file, index_col=1).index\n",
    "\n",
    "        warn_file = warn_file_list[-1]\n",
    "        warn_tot = pd.read_csv(target_path + '/' + selected_column + '/' + warn_file, index_col=1).index\n",
    "\n",
    "        # Long-term Crit/Warn for selected item\n",
    "        sub_df = data_long_fixed[['DriveSerialNumber', 'date', selected_column]]\n",
    "        sub_df = sub_df.drop_duplicates(subset=['DriveSerialNumber', 'date'], keep='last')\n",
    "\n",
    "        sub_df_pivot = sub_df.pivot(index='DriveSerialNumber', columns='date', values=selected_column)\n",
    "        xdate_tot = pd.Series(pd.unique(data_long_fixed['date']))\n",
    "        xdate_tot = xdate_tot.sort_values()\n",
    "        xdate_tot = xdate_tot.reset_index(drop=True)\n",
    "\n",
    "        sub_df_crit_tot = sub_df_pivot[sub_df_pivot.index.isin(crit_tot)]\n",
    "        sub_df_warn_tot = sub_df_pivot[sub_df_pivot.index.isin(warn_tot)]\n",
    "\n",
    "        # Short-term Crit/Warn for selected item\n",
    "        sub_df_short = data_short[['DriveSerialNumber', 'date', selected_column]]\n",
    "        sub_df_short = sub_df_short.drop_duplicates(subset=['DriveSerialNumber', 'date'], keep='last')\n",
    "\n",
    "        sub_df_pivot_short = sub_df_short.pivot(index='DriveSerialNumber', columns='date', values=selected_column)\n",
    "        xdate_short = pd.Series(pd.unique(data_short['date']))\n",
    "        xdate_short = xdate_short.sort_values()\n",
    "        xdate_short = xdate_short.reset_index(drop=True)\n",
    "\n",
    "        sub_df_crit_short = sub_df_pivot_short[sub_df_pivot_short.index.isin(crit_tot)]\n",
    "        sub_df_warn_short = sub_df_pivot_short[sub_df_pivot_short.index.isin(warn_tot)]\n",
    "\n",
    "        # Filter Crit SN - Rulebase 정보 사용\n",
    "        crit_low = tmp_info.CRITICAL_LOW.split(',')\n",
    "        crit_high = tmp_info.CRITICAL_HIGH.split(',')\n",
    "        if crit_low[0] == 'VALUE':\n",
    "            if crit_high[0] == 'VALUE':\n",
    "                sub_df_crit_temp = sub_df_crit_short[(sub_df_crit_short < float(crit_low[1]))\n",
    "                                                     | (sub_df_crit_short > float(crit_high[1]))]\n",
    "            else:\n",
    "                sub_df_crit_temp = sub_df_crit_short[(sub_df_crit_short < float(crit_low[1]))]\n",
    "        elif crit_high[0] == 'VALUE':\n",
    "            sub_df_crit_temp = sub_df_crit_short[(sub_df_crit_short > float(crit_high[1]))]\n",
    "        else:  # Crit 조건 없을 때\n",
    "            sub_df_crit_temp = sub_df_crit_short\n",
    "\n",
    "        sub_df_crit_temp = sub_df_crit_temp.dropna(axis=0, how='all')\n",
    "        crit_tot_new = sub_df_crit_temp.index\n",
    "\n",
    "        sub_df_crit_short = sub_df_crit_short.loc[crit_tot_new]\n",
    "        sub_df_crit_tot = sub_df_crit_tot.loc[crit_tot_new]\n",
    "\n",
    "        # Filter Warn SN - 별도 추출한 Warning boundary 정보 적용\n",
    "        boundary_file = boundary_file_list[-1]\n",
    "        boundary = pd.read_csv(target_path +'/' + selected_column + '/' + boundary_file)\n",
    "\n",
    "        warn_low = tmp_info.WARNING_LOW.split(',')\n",
    "        warn_high = tmp_info.WARNING_HIGH.split(',')\n",
    "\n",
    "        sub_df_warn_temp = sub_df_warn_short.copy()\n",
    "\n",
    "        if warn_low[0] != 'EMPTY':\n",
    "            if warn_high[0] != 'EMPTY':\n",
    "                for i in range(len(sub_df_warn_temp.columns)):\n",
    "                    warn_col_temp = sub_df_warn_short.iloc[:, i]\n",
    "                    sub_df_warn_temp.iloc[:, i] = warn_col_temp[(warn_col_temp < boundary['Lower'][i])\n",
    "                                                                | (warn_col_temp > boundary['Upper'][i])]\n",
    "            else:\n",
    "                for i in range(len(sub_df_warn_temp.columns)):\n",
    "                    warn_col_temp = sub_df_warn_short.iloc[:, i]\n",
    "                    sub_df_warn_temp.iloc[:, i] = warn_col_temp[warn_col_temp < boundary['Lower'][i]]\n",
    "        elif warn_high[0] != 'EMPTY':\n",
    "            for i in range(len(sub_df_warn_temp.columns)):\n",
    "                warn_col_temp = sub_df_warn_short.iloc[:, i]\n",
    "                sub_df_warn_temp.iloc[:, i] = warn_col_temp[warn_col_temp > boundary['Upper'][i]]\n",
    "\n",
    "        sub_df_warn_temp = sub_df_warn_temp.dropna(axis=0, how='all')\n",
    "        warn_tot_new = sub_df_warn_temp.index\n",
    "\n",
    "        sub_df_warn_short = sub_df_warn_short.loc[warn_tot_new]\n",
    "        sub_df_warn_tot = sub_df_warn_tot.loc[warn_tot_new]\n",
    "\n",
    "        print('warn length (org) : {}'.format(len(warn_tot)))\n",
    "        print('warn length (fix) : {}'.format(len(warn_tot_new)))\n",
    "\n",
    "        ## Last Week Abnormal Data Save (Excel File) #############################\n",
    "\n",
    "        sub_df_raw = data_short.set_index('DriveSerialNumber')\n",
    "        sub_df_crit_raw = sub_df_raw[sub_df_raw.index.isin(crit_tot_new)]\n",
    "        sub_df_crit_raw.to_csv(target_path + '/0_Abnormal_Data/Crit_' + selected_column + '.csv')\n",
    "        sub_df_warn_raw = sub_df_raw[sub_df_raw.index.isin(warn_tot_new)]\n",
    "        sub_df_warn_raw.to_csv(target_path + '/0_Abnormal_Data/Warn_' + selected_column + '.csv')\n",
    "\n",
    "        ##########################################################################\n",
    "\n",
    "        day = weight_tot.shape[0]\n",
    "        n = weight_tot.shape[1]\n",
    "\n",
    "        day_short = weight_short.shape[0]\n",
    "        n_short = weight_short.shape[1]\n",
    "\n",
    "        tmp = [[weight_tot[j, i + 1] - weight_tot[j, i] for i in range(n - 1)] for j in range(day - 1)]\n",
    "        if not np.isnan(tmp).all():\n",
    "            normalize = np.nanmax(tmp)  # 전체 weight 중 NaN 제외한 최대값\n",
    "            weight_tot = np.nan_to_num(weight_tot)  # weight 중 일부만 NaN일 경우 해당 값들을 0으로 초기화\n",
    "        else:\n",
    "            normalize = np.nan  # 모든 값이 NaN일 경우 NaN\n",
    "\n",
    "        tmp_short = [[weight_short[j, i + 1] - weight_short[j, i] for i in range(n_short - 1)] for j in\n",
    "                     range(day_short - 1)]\n",
    "        if not np.isnan(tmp_short).all():\n",
    "            normalize_short = np.nanmax(tmp_short)\n",
    "            weight_short = np.nan_to_num(weight_short)\n",
    "        else:\n",
    "            normalize_short = np.nan\n",
    "\n",
    "        if tmp_info.Y_FORMAT == 'TB':\n",
    "            value_tot = value_tot * 512 * 1000 / (1024 ** 4)\n",
    "            value_short = value_short * 512 * 1000 / (1024 ** 4)\n",
    "\n",
    "            sub_df_crit_tot = sub_df_crit_tot * 512 * 1000 / (1024 ** 4)\n",
    "            sub_df_crit_short = sub_df_crit_short * 512 * 1000 / (1024 ** 4)\n",
    "\n",
    "            sub_df_warn_tot = sub_df_warn_tot * 512 * 1000 / (1024 ** 4)\n",
    "            sub_df_warn_short = sub_df_warn_short * 512 * 1000 / (1024 ** 4)\n",
    "\n",
    "        heatmap_color1 = np.zeros((day - 1, n - 1))\n",
    "        heatmap_opacity = np.zeros((day - 1, n - 1))\n",
    "\n",
    "        ## axis zoom range 설정:\n",
    "        # upper bound가 주어지지 않은 항목들에 대한 조건 설정 및\n",
    "        # upper bound가 주어진 경우 실제 density heatmap 구간의 최대값과 비교하여 axis_max 설정\n",
    "        # Long / Short 간 zoom in 구간 분리\n",
    "        value_max_tot = np.amax(value_tot)  # density heatmap 구간의 최대값\n",
    "        if not np.isnan(tmp_info.Y_MAX):\n",
    "            if selected_column.startswith('Temp') == True:  # 온도의 경우 항상 지정된 범위로 설정\n",
    "                value_max_tot = tmp_info.Y_MAX\n",
    "            else:\n",
    "                value_max_tot = min(value_max_tot, tmp_info.Y_MAX)\n",
    "        else:\n",
    "            if value_max_tot <= tmp_info.Y_MIN:  # density 구간 고정이며 0보다 작거나 같은 경우\n",
    "                value_max_tot = 200\n",
    "        axis_max_tot = value_max_tot + (value_max_tot - tmp_info.Y_MIN) * 0.1\n",
    "        axis_min_tot = tmp_info.Y_MIN - (value_max_tot - tmp_info.Y_MIN) * 0.1\n",
    "\n",
    "        value_max_short = np.amax(value_short)  # density heatmap 구간의 최대값\n",
    "        if not np.isnan(tmp_info.Y_MAX):\n",
    "            if selected_column.startswith('Temp') == True:  # 온도의 경우 항상 지정된 범위로 설정\n",
    "                value_max_short = tmp_info.Y_MAX\n",
    "            else:\n",
    "                value_max_short = min(value_max_short, tmp_info.Y_MAX)\n",
    "        else:\n",
    "            if value_max_short <= tmp_info.Y_MIN:  # density 구간 고정이며 0보다 작거나 같은 경우\n",
    "                value_max_short = 200\n",
    "        axis_max_short = value_max_short + (value_max_short - tmp_info.Y_MIN) * 0.1\n",
    "        axis_min_short = tmp_info.Y_MIN - (value_max_short - tmp_info.Y_MIN) * 0.1\n",
    "\n",
    "        ##Long Term Plot (ax1 : Whole, ax2 : Zoom in)\n",
    "        fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, sharex=True, figsize=(12, 3), dpi=100)\n",
    "\n",
    "        # Plot for warning SSDs\n",
    "        for ind in sub_df_warn_tot.index:\n",
    "            tmp_warn = sub_df_warn_tot.loc[ind]\n",
    "            tmp_warn_df = pd.DataFrame(\n",
    "                {'tmp_warn_x': np.arange(0, day, 1), 'tmp_warn_y': tmp_warn[:tmp_warn.size]}).dropna()\n",
    "\n",
    "            if len(tmp_warn_df) == 1:  # Use marker if only one value exists\n",
    "                ax1.plot(pd.to_datetime(tmp_warn_df.index), tmp_warn_df.tmp_warn_y, color='grey', linewidth=0.5,\n",
    "                         zorder=0,\n",
    "                         marker='o', markersize=1)\n",
    "            else:\n",
    "                ax1.plot(pd.to_datetime(tmp_warn_df.index), tmp_warn_df.tmp_warn_y, color='grey', linewidth=0.5,\n",
    "                         zorder=0)\n",
    "\n",
    "            if (selected_column.startswith('Temp') == True or\n",
    "                    selected_column.startswith('Avail') == True or\n",
    "                    np.isnan(normalize) == True):\n",
    "                if len(tmp_warn_df) == 1:  # Use marker if only one value exists\n",
    "                    ax2.plot(pd.to_datetime(tmp_warn_df.index), tmp_warn_df.tmp_warn_y, color='grey', linewidth=0.5,\n",
    "                             zorder=0,\n",
    "                             marker='o', markersize=1)\n",
    "                else:\n",
    "                    ax2.plot(pd.to_datetime(tmp_warn_df.index), tmp_warn_df.tmp_warn_y, color='grey', linewidth=0.5,\n",
    "                             zorder=0)\n",
    "\n",
    "        # Plot for critical SSDs\n",
    "        for ind in sub_df_crit_tot.index:\n",
    "            tmp_crit = sub_df_crit_tot.loc[ind]\n",
    "            tmp_crit_df = pd.DataFrame(\n",
    "                {'tmp_crit_x': np.arange(0, day, 1), 'tmp_crit_y': tmp_crit[:tmp_crit.size]}).dropna()\n",
    "            if len(tmp_crit_df) == 1:  # Use marker if only one value exists\n",
    "                ax1.plot(pd.to_datetime(tmp_crit_df.index), tmp_crit_df.tmp_crit_y, color='red', linewidth=0.5,\n",
    "                         zorder=20,\n",
    "                         marker='o', markersize=1)\n",
    "            else:\n",
    "                ax1.plot(pd.to_datetime(tmp_crit_df.index), tmp_crit_df.tmp_crit_y, color='red', linewidth=0.5,\n",
    "                         zorder=20)\n",
    "\n",
    "            if (selected_column.startswith('Temp') == True or\n",
    "                    selected_column.startswith('Avail') == True or\n",
    "                    np.isnan(normalize) == True):\n",
    "                if len(tmp_crit_df) == 1:  # Use marker if only one value exists\n",
    "                    ax2.plot(pd.to_datetime(tmp_crit_df.index), tmp_crit_df.tmp_crit_y, color='red', linewidth=0.5,\n",
    "                             zorder=20,\n",
    "                             marker='o', markersize=1)\n",
    "                else:\n",
    "                    ax2.plot(pd.to_datetime(tmp_crit_df.index), tmp_crit_df.tmp_crit_y, color='red', linewidth=0.5,\n",
    "                             zorder=20)\n",
    "\n",
    "        # Heatmap plot\n",
    "        if not np.isnan(normalize):  # Density 존재하지 않는 항목에 대해 heatmap 그리지 않도록 함\n",
    "            for j in range(day - 1):\n",
    "                for i in range(n - 1):\n",
    "                    heatmap_color1[j, i] = ((1 - (weight_tot[j, i + 1] - weight_tot[\n",
    "                        j, i]) / normalize)) * 0.8 + 0.2  # darkblue 0.2 ~ yellow 1.0\n",
    "                    heatmap_opacity[j, i] = (weight_tot[j, i + 1] - weight_tot[j, i]) / normalize\n",
    "                    ax1.fill_between((xdate_tot[j], xdate_tot[j + 1]), value_tot[(j, j + 1), i],\n",
    "                                     value_tot[(j, j + 1), (i + 1)],\n",
    "                                     facecolor=colormap1(heatmap_color1[j, i])[:3] + (\n",
    "                                     heatmap_opacity[j, i] * 0.9 + 0.1,),\n",
    "                                     edgecolor=colormap1(heatmap_color1[j, i])[:3] + (\n",
    "                                     heatmap_opacity[j, i] * 0.9 + 0.1,), zorder=15)\n",
    "\n",
    "                    ax2.fill_between((xdate_tot[j], xdate_tot[j + 1]), value_tot[(j, j + 1), i],\n",
    "                                     value_tot[(j, j + 1), (i + 1)],\n",
    "                                     facecolor=colormap1(heatmap_color1[j, i])[:3] + (\n",
    "                                     heatmap_opacity[j, i] * 0.9 + 0.1,),\n",
    "                                     edgecolor=colormap1(heatmap_color1[j, i])[:3] + (\n",
    "                                     heatmap_opacity[j, i] * 0.9 + 0.1,), zorder=15)\n",
    "\n",
    "        # Warn / Crit / Heatmap 모두 존재하지 않을 경우\n",
    "        elif (sub_df_crit_tot.size == 0) and (sub_df_warn_tot.size == 0) and np.isnan(normalize):\n",
    "            if selected_column.startswith('Avail') == True:\n",
    "                ax1.plot(xdate_tot, [100 for i in range(len(xdate_tot))], color='black', linewidth=1)\n",
    "                ax2.plot(xdate_tot, [100 for i in range(len(xdate_tot))], color='black', linewidth=1)\n",
    "            else:\n",
    "                ax1.plot(xdate_tot, [0 for i in range(len(xdate_tot))], color='black', linewidth=1)\n",
    "                ax2.plot(xdate_tot, [0 for i in range(len(xdate_tot))], color='black', linewidth=1)\n",
    "\n",
    "        ax1.set_title(selected_column + ' (Whole Trend)', fontsize=15)\n",
    "        ax1.tick_params(labelsize=10)\n",
    "        ax1.set_xlabel('Date', fontsize=13)\n",
    "        if tmp_info.Y_FORMAT == 'TB':\n",
    "            ax1.set_ylabel(selected_column + ' (TB)', fontsize=13)\n",
    "        else:\n",
    "            ax1.set_ylabel(selected_column, fontsize=13)\n",
    "\n",
    "        ax2.set_title(selected_column + ' (Zoom In)', fontsize=15)\n",
    "        ax2.tick_params(labelsize=10)\n",
    "        ax2.set_xlabel('Date', fontsize=13)\n",
    "        if tmp_info.Y_FORMAT == 'TB':\n",
    "            ax2.set_ylabel(selected_column + ' (TB)', fontsize=13)\n",
    "        else:\n",
    "            ax2.set_ylabel(selected_column, fontsize=13)\n",
    "        ax2.set_ylim(axis_min_tot, axis_max_tot)\n",
    "\n",
    "        # y축 상 과도하게 큰 숫자는 scientific format으로 변경\n",
    "        ax1.ticklabel_format(style='sci', axis='y', scilimits=(-3, 6), useOffset=False)\n",
    "        ax2.ticklabel_format(style='sci', axis='y', scilimits=(-3, 6), useOffset=False)\n",
    "\n",
    "        # Host Read/Write의 Million format은 위의 scientific format으로 대체\n",
    "        #     ax1.ticklabel_format(style = 'plain', axis='y', useOffset=False)\n",
    "        #     ax2.ticklabel_format(style = 'plain', axis='y', useOffset=False)\n",
    "\n",
    "        #     if tmp_info.Y_FORMAT == \"M\":\n",
    "        #         formatter = FuncFormatter(millions)\n",
    "        #         ax1.yaxis.set_major_formatter(formatter)\n",
    "        #         ax2.yaxis.set_major_formatter(formatter)\n",
    "\n",
    "        # Customize xticks for long-term plot\n",
    "        xticks = ax1.get_xticks()\n",
    "        ax1.set_xticks(np.linspace(xticks[0], xticks[-1], 7))\n",
    "\n",
    "        ax1.xaxis.set_major_formatter(DateFormatter('%b %d'))\n",
    "        ax2.xaxis.set_major_formatter(DateFormatter('%b %d'))\n",
    "\n",
    "        fig.tight_layout()\n",
    "        fig.savefig(target_path +'/1_Figure/' + selected_column + '_LongTerm_Trend.png', dpi=100)\n",
    "\n",
    "        plt.close(fig)\n",
    "        fig.clf()\n",
    "\n",
    "        ##Short Term\n",
    "        fig_short, (ax3, ax4) = plt.subplots(nrows=1, ncols=2, sharex=True, figsize=(12, 4.6), dpi=100)\n",
    "\n",
    "        # Plot for warning SSDs\n",
    "        for ind in sub_df_warn_short.index:\n",
    "            tmp_warn = sub_df_warn_short.loc[ind]\n",
    "            tmp_warn_df = pd.DataFrame(\n",
    "                {'tmp_warn_x': np.arange(0, day_short, 1), 'tmp_warn_y': tmp_warn[:tmp_warn.size]}).dropna()\n",
    "            if len(tmp_warn_df) == 1:  # Use marker if only one value exists\n",
    "                ax3.plot(pd.to_datetime(tmp_warn_df.index), tmp_warn_df.tmp_warn_y, color='grey', linewidth=0.5,\n",
    "                         zorder=0,\n",
    "                         marker='o', markersize=1)\n",
    "            else:\n",
    "                ax3.plot(pd.to_datetime(tmp_warn_df.index), tmp_warn_df.tmp_warn_y, color='grey', linewidth=0.5,\n",
    "                         zorder=0)\n",
    "\n",
    "            if (selected_column.startswith('Temp') == True or\n",
    "                    selected_column.startswith('Avail') == True or\n",
    "                    np.isnan(normalize_short) == True):\n",
    "                if len(tmp_warn_df) == 1:  # Use marker if only one value exists\n",
    "                    ax4.plot(pd.to_datetime(tmp_warn_df.index), tmp_warn_df.tmp_warn_y, color='grey', linewidth=0.5,\n",
    "                             zorder=0,\n",
    "                             marker='o', markersize=1)\n",
    "                else:\n",
    "                    ax4.plot(pd.to_datetime(tmp_warn_df.index), tmp_warn_df.tmp_warn_y, color='grey', linewidth=0.5,\n",
    "                             zorder=0)\n",
    "\n",
    "        # Plot for critical SSDs\n",
    "        for ind in sub_df_crit_short.index:\n",
    "            tmp_crit = sub_df_crit_short.loc[ind]\n",
    "            tmp_crit_df = pd.DataFrame(\n",
    "                {'tmp_crit_x': np.arange(0, day_short, 1), 'tmp_crit_y': tmp_crit[:tmp_crit.size]}).dropna()\n",
    "            if len(tmp_crit_df) == 1:  # Use marker if only one value exists\n",
    "                ax3.plot(pd.to_datetime(tmp_crit_df.index), tmp_crit_df.tmp_crit_y, color='red', linewidth=0.5,\n",
    "                         zorder=20,\n",
    "                         marker='o', markersize=1)\n",
    "            else:\n",
    "                ax3.plot(pd.to_datetime(tmp_crit_df.index), tmp_crit_df.tmp_crit_y, color='red', linewidth=0.5,\n",
    "                         zorder=20)\n",
    "\n",
    "            if (selected_column.startswith('Temp') == True or\n",
    "                    selected_column.startswith('Avail') == True or\n",
    "                    np.isnan(normalize_short) == True):\n",
    "                if len(tmp_crit_df) == 1:  # Use marker if only one value exists\n",
    "                    ax4.plot(pd.to_datetime(tmp_crit_df.index), tmp_crit_df.tmp_crit_y, color='red', linewidth=0.5,\n",
    "                             zorder=20,\n",
    "                             marker='o', markersize=1)\n",
    "                else:\n",
    "                    ax4.plot(pd.to_datetime(tmp_crit_df.index), tmp_crit_df.tmp_crit_y, color='red', linewidth=0.5,\n",
    "                             zorder=20)\n",
    "\n",
    "        # Heatmap plot\n",
    "        if not np.isnan(normalize_short):  # Density 존재하지 않는 항목에 대해 heatmap 그리지 않도록 함\n",
    "            for j in range(day_short - 1):\n",
    "                for i in range(n_short - 1):\n",
    "                    heatmap_color1[j, i] = ((1 - (weight_short[j, i + 1] - weight_short[\n",
    "                        j, i]) / normalize_short)) * 0.8 + 0.2  # darkblue 0.2 ~ yellow 1.0\n",
    "                    heatmap_opacity[j, i] = (weight_short[j, i + 1] - weight_short[j, i]) / normalize_short\n",
    "\n",
    "                    ax3.fill_between((xdate_short[j], xdate_short[j + 1]), value_short[(j, j + 1), i],\n",
    "                                     value_short[(j, j + 1), (i + 1)],\n",
    "                                     facecolor=colormap1(heatmap_color1[j, i])[:3] + (\n",
    "                                     heatmap_opacity[j, i] * 0.9 + 0.1,),\n",
    "                                     edgecolor=colormap1(heatmap_color1[j, i])[:3] + (\n",
    "                                     heatmap_opacity[j, i] * 0.9 + 0.1,), zorder=15)\n",
    "\n",
    "                    ax4.fill_between((xdate_short[j], xdate_short[j + 1]), value_short[(j, j + 1), i],\n",
    "                                     value_short[(j, j + 1), (i + 1)],\n",
    "                                     facecolor=colormap1(heatmap_color1[j, i])[:3] + (\n",
    "                                     heatmap_opacity[j, i] * 0.9 + 0.1,),\n",
    "                                     edgecolor=colormap1(heatmap_color1[j, i])[:3] + (\n",
    "                                     heatmap_opacity[j, i] * 0.9 + 0.1,), zorder=15)\n",
    "\n",
    "        # Warn / Crit / Heatmap 모두 존재하지 않을 경우\n",
    "        elif (sub_df_crit_short.size == 0) and (sub_df_warn_short.size == 0) and np.isnan(normalize_short):\n",
    "            if selected_column.startswith('Avail') == True:\n",
    "                ax3.plot(xdate_short, [100 for i in range(len(xdate_short))], color='black', linewidth=1)\n",
    "                ax4.plot(xdate_short, [100 for i in range(len(xdate_short))], color='black', linewidth=1)\n",
    "            else:\n",
    "                ax3.plot(xdate_short, [0 for i in range(len(xdate_short))], color='black', linewidth=1)\n",
    "                ax4.plot(xdate_short, [0 for i in range(len(xdate_short))], color='black', linewidth=1)\n",
    "\n",
    "        ax3.set_title(selected_column + ' (Whole Trend)', fontsize=16)\n",
    "        ax3.tick_params(labelsize=11)\n",
    "        ax3.set_xlabel('Date', fontsize=14)\n",
    "        if tmp_info.Y_FORMAT == 'TB':\n",
    "            ax3.set_ylabel(selected_column + ' (TB)', fontsize=14)\n",
    "        else:\n",
    "            ax3.set_ylabel(selected_column, fontsize=14)\n",
    "\n",
    "        ax4.set_title(selected_column + ' (Zoom In)', fontsize=16)\n",
    "        ax4.tick_params(labelsize=11)\n",
    "        ax4.set_xlabel('Date', fontsize=14)\n",
    "        if tmp_info.Y_FORMAT == 'TB':\n",
    "            ax4.set_ylabel(selected_column + ' (TB)', fontsize=14)\n",
    "        else:\n",
    "            ax4.set_ylabel(selected_column, fontsize=14)\n",
    "        ax4.set_ylim(axis_min_short, axis_max_short)\n",
    "\n",
    "        # y축 상 과도하게 큰 숫자는 scientific format으로 변경\n",
    "        ax3.ticklabel_format(style='sci', axis='y', scilimits=(-3, 6), useOffset=False)\n",
    "        ax4.ticklabel_format(style='sci', axis='y', scilimits=(-3, 6), useOffset=False)\n",
    "\n",
    "        # Host Read/Write의 Million format은 위의 scientific format으로 대체\n",
    "        #     ax3.ticklabel_format(style = 'plain', axis='y', useOffset=False)\n",
    "        #     ax4.ticklabel_format(style = 'plain', axis='y', useOffset=False)\n",
    "\n",
    "        #     if tmp_info.Y_FORMAT == \"M\":\n",
    "        #         formatter = FuncFormatter(millions)\n",
    "        #         ax3.yaxis.set_major_formatter(formatter)\n",
    "        #         ax4.yaxis.set_major_formatter(formatter)\n",
    "\n",
    "        # Customize xticks for short-term plot\n",
    "        xticks = ax3.get_xticks()\n",
    "        day_count = int(xticks[-1] - xticks[0]) + 1\n",
    "        ax3.set_xticks(np.linspace(xticks[0], xticks[-1], day_count))\n",
    "\n",
    "        ax3.xaxis.set_major_formatter(DateFormatter('%m-%d'))\n",
    "        ax4.xaxis.set_major_formatter(DateFormatter('%m-%d'))\n",
    "\n",
    "        fig_short.tight_layout()\n",
    "        fig_short.savefig(target_path +'/1_Figure/' + selected_column + '_ShortTerm_Trend.png', dpi=100)\n",
    "\n",
    "        plt.close(fig_short)\n",
    "        fig_short.clf()\n",
    "\n",
    "        end = time.time()\n",
    "        print('Processing {} done - Elapsed time: '.format(selected_column) +\n",
    "              f'{(end-start):.3f} seconds')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #### Visualization - Critical Warning / Capacitor Health (Short Term)\n",
    "    # items - SMART: Critical Warning, Ext_SMART: Capacitor Health\n",
    "    # item 이름 및 정상 조건\n",
    "    if product in ['PM963', 'PM983', 'PM953']:\n",
    "        item_sp = ['CritWarning', 'Capacitor_Health']\n",
    "    else:\n",
    "        item_sp = ['CritWarning']\n",
    "\n",
    "    cols_sp = system_info + ['SSDUID'] + item_sp\n",
    "\n",
    "    # 먼저 last data 중에서 Critical SN 목록을 정리\n",
    "    print('Loading last data file {}: '.format(file_list[-1]) + time.strftime('%Y-%m-%d %H:%M:%S', time.localtime()))\n",
    "    start = time.time()\n",
    "\n",
    "    sn_critwarning = pd.Series(dtype='object')\n",
    "    sn_capfail = pd.Series(dtype='object')\n",
    "    for chunk in pd.read_csv(path + '/' + file_list[-1], chunksize=1000000, usecols=cols_sp, dtype=types_dict):\n",
    "        chunk_critwarning = pd.Series(dtype='object')\n",
    "        chunk_capfail = pd.Series(dtype='object')\n",
    "        for item in item_sp:\n",
    "            if item == 'CritWarning':\n",
    "                default = 0\n",
    "                item_crit = pd.Series(\n",
    "                    chunk[(~chunk[item].isna()) & (chunk[item] != default)]['DriveSerialNumber'].unique())\n",
    "                chunk_critwarning = pd.concat([chunk_critwarning, item_crit])\n",
    "            elif item == 'Capacitor_Health':\n",
    "                default = 100\n",
    "                item_crit = pd.Series(\n",
    "                    chunk[(~chunk[item].isna()) & (chunk[item] != default)]['DriveSerialNumber'].unique())\n",
    "                chunk_capfail = pd.concat([chunk_capfail, item_crit])\n",
    "\n",
    "        sn_critwarning = pd.concat([sn_critwarning, chunk_critwarning])\n",
    "        sn_capfail = pd.concat([sn_capfail, chunk_capfail])\n",
    "\n",
    "    sn_critwarning = pd.Series(sn_critwarning.unique())\n",
    "    sn_capfail = pd.Series(sn_capfail.unique())\n",
    "\n",
    "    sn_sp_total = pd.Series(pd.concat([sn_critwarning, sn_capfail]).unique())\n",
    "\n",
    "    end = time.time()\n",
    "    print(f'Elapsed time : {(end-start):.3f} seconds')\n",
    "\n",
    "\n",
    "    # 모든 데이터들에서 Critical SN DataFrame 불러오기\n",
    "    start = time.time()\n",
    "    data_sp = pd.DataFrame()\n",
    "\n",
    "    for file in file_list:\n",
    "        print('Loading filename {}: '.format(file) + time.strftime('%Y-%m-%d %H:%M:%S', time.localtime()))\n",
    "        tmp_data = pd.DataFrame()\n",
    "        for chunk in pd.read_csv(path + '/' + file, chunksize=1000000, dtype=types_dict):\n",
    "            chunk = chunk[chunk['DriveSerialNumber'].isin(sn_sp_total)]\n",
    "            chunk['date'] = chunk.TIMESTAMP.str[:10]\n",
    "            tmp_data = pd.concat([tmp_data, chunk])\n",
    "        tmp_data = tmp_data.sort_values(by=['DriveSerialNumber', 'TIMESTAMP'])  # Sort by: DriveSerialNumber, TIMESTAMP\n",
    "\n",
    "        data_sp = pd.concat([data_sp, tmp_data])\n",
    "    data_sp = data_sp.sort_values(by=['DriveSerialNumber', 'TIMESTAMP'])  # Sort by: DriveSerialNumber, TIMESTAMP\n",
    "    end = time.time()\n",
    "    print(f'Elapsed time : {(end-start):.3f} seconds')\n",
    "\n",
    "    # groupby 사용해서 각 DriveSerialNumber에 대해 유일한 SSDUID만 남기기\n",
    "    grouped = data_sp.groupby(by='DriveSerialNumber')\n",
    "    data_sp_long_fixed = grouped.apply(get_df_1ssduid)\n",
    "    data_sp_long_fixed = data_sp_long_fixed.reset_index(drop=True)\n",
    "\n",
    "    short_datetime = tmp_data['date'].astype('datetime64[ns]').min()\n",
    "\n",
    "    # Generate short-term Crit/Warn data\n",
    "    data_sp_long_fixed['date'] = pd.to_datetime(data_sp_long_fixed['date'])\n",
    "    data_sp_short = data_sp_long_fixed[data_sp_long_fixed['date'] >= short_datetime]\n",
    "\n",
    "    ## Abnormal Data Save & Short Term Plot\n",
    "    for item in item_sp:\n",
    "        sub_df = data_sp_short[['DriveSerialNumber', 'date', item]]\n",
    "        sub_df = sub_df.drop_duplicates(subset=['DriveSerialNumber', 'date'], keep='last')\n",
    "\n",
    "        sub_df_pivot = sub_df.pivot(index='DriveSerialNumber', columns='date', values=item)\n",
    "\n",
    "        if item == 'CritWarning':\n",
    "            crit_tot = sn_critwarning\n",
    "            default = 0\n",
    "        elif item == 'Capacitor_Health':\n",
    "            crit_tot = sn_capfail\n",
    "            default = 100\n",
    "\n",
    "        sub_df_crit = sub_df_pivot[sub_df_pivot.index.isin(crit_tot)]\n",
    "\n",
    "        # Filter Crit SN\n",
    "        sub_df_crit_temp = sub_df_crit[sub_df_crit != default]\n",
    "\n",
    "        sub_df_crit_temp = sub_df_crit_temp.dropna(axis=0, how='all')\n",
    "        crit_tot_new = sub_df_crit_temp.index\n",
    "\n",
    "        print('Before filtering: {}'.format(len(crit_tot)))\n",
    "        print('After filtering: {}'.format(len(crit_tot_new)))\n",
    "\n",
    "        sub_df_crit = sub_df_crit.loc[crit_tot_new]\n",
    "\n",
    "        ## Last Week Abnormal Data Save (Excel File) #############################\n",
    "\n",
    "        sub_df_raw = data_sp_short.set_index('DriveSerialNumber')\n",
    "        sub_df_crit_raw = sub_df_raw[sub_df_raw.index.isin(crit_tot_new)]\n",
    "        sub_df_crit_raw.to_csv(target_path +'/0_Abnormal_Data/Crit_' + item + '.csv')\n",
    "\n",
    "        ##########################################################################\n",
    "\n",
    "        fig, axw = plt.subplots(nrows=1, ncols=1, sharex=True, figsize=(6, 4.6), dpi=100)\n",
    "        for ind in sub_df_crit.index:\n",
    "            tmp_crit = sub_df_crit.loc[ind]\n",
    "            tmp_crit_df = pd.DataFrame({'tmp_crit_y': tmp_crit[:tmp_crit.size]})\n",
    "            axw.plot(pd.to_datetime(tmp_crit_df.index), tmp_crit_df.tmp_crit_y, color='red', linewidth=0.5, zorder=20)\n",
    "        axw.plot(pd.to_datetime(tmp_crit_df.index), [default for i in range(len(tmp_crit_df.index))],\n",
    "                 color='black', linewidth=1, zorder=25)\n",
    "        axw.xaxis.set_major_formatter(DateFormatter('%m-%d'))\n",
    "\n",
    "        axw.set_title('{} (Whole Trend)'.format(item), fontsize=16)\n",
    "        axw.tick_params(labelsize=11)\n",
    "        axw.set_xlabel('Date', fontsize=14)\n",
    "        axw.set_ylabel(item, fontsize=14)\n",
    "\n",
    "        if item == 'CritWarning':\n",
    "            axw.set_ylim(-0.5, 33)\n",
    "            axw.set_yticks((0, 2, 4, 8, 16, 32))\n",
    "\n",
    "        # Customize xticks for short-term plot\n",
    "        xticks = axw.get_xticks()\n",
    "        day_count = int(xticks[-1] - xticks[0]) + 1\n",
    "        axw.set_xticks(np.linspace(xticks[0], xticks[-1], day_count))\n",
    "\n",
    "        fig.tight_layout()\n",
    "        fig.savefig(target_path +'/1_Figure/{}_ShortTerm_Trend.png'.format(item), dpi=100)\n",
    "\n",
    "    ## Long Term Plot (Optional)\n",
    "    for item in item_sp:\n",
    "        sub_df = data_sp_long_fixed[['DriveSerialNumber', 'date', item]]\n",
    "        sub_df = sub_df.drop_duplicates(subset=['DriveSerialNumber', 'date'], keep='last')\n",
    "\n",
    "        sub_df_pivot = sub_df.pivot(index='DriveSerialNumber', columns='date', values=item)\n",
    "\n",
    "        if item == 'CritWarning':\n",
    "            crit_tot = sn_critwarning\n",
    "            default = 0\n",
    "        elif item == 'Capacitor_Health':\n",
    "            crit_tot = sn_capfail\n",
    "            default = 100\n",
    "\n",
    "        sub_df_crit = sub_df_pivot[sub_df_pivot.index.isin(crit_tot)]\n",
    "\n",
    "        # Filter Crit SN\n",
    "        sub_df_crit_temp = sub_df_crit[sub_df_crit != default]\n",
    "\n",
    "        sub_df_crit_temp = sub_df_crit_temp.dropna(axis=0, how='all')\n",
    "        crit_tot_new = sub_df_crit_temp.index\n",
    "\n",
    "        print('Before filtering: {}'.format(len(crit_tot)))\n",
    "        print('After filtering: {}'.format(len(crit_tot_new)))\n",
    "\n",
    "        sub_df_crit = sub_df_crit.loc[crit_tot_new]\n",
    "\n",
    "        fig, axw = plt.subplots(nrows=1, ncols=1, sharex=True, figsize=(6, 3), dpi=100)\n",
    "        for ind in sub_df_crit.index:\n",
    "            tmp_crit = sub_df_crit.loc[ind]\n",
    "            tmp_crit_df = pd.DataFrame({'tmp_crit_y': tmp_crit[:tmp_crit.size]})\n",
    "            axw.plot(pd.to_datetime(tmp_crit_df.index), tmp_crit_df.tmp_crit_y, color='red', linewidth=0.5, zorder=20)\n",
    "        axw.plot(pd.to_datetime(tmp_crit_df.index), [default for i in range(len(tmp_crit_df.index))],\n",
    "                 color='black', linewidth=1, zorder=25)\n",
    "        axw.xaxis.set_major_formatter(DateFormatter('%m-%d'))\n",
    "\n",
    "        axw.set_title('{} (Whole Trend)'.format(item), fontsize=16)\n",
    "        axw.tick_params(labelsize=11)\n",
    "        axw.set_xlabel('Date', fontsize=14)\n",
    "        axw.set_ylabel(item, fontsize=14)\n",
    "\n",
    "        if item == 'CritWarning':\n",
    "            axw.set_ylim(-0.5, 33)\n",
    "            axw.set_yticks((0, 2, 4, 8, 16, 32))\n",
    "\n",
    "        fig.tight_layout()\n",
    "        fig.savefig(target_path + '/1_Figure/{}_LongTerm_Trend.png'.format(item), dpi=100)\n",
    "\n",
    "\n",
    "###### Visualization (Histogram)\n",
    "def Visualization_hist(target_product, target_logitem):\n",
    "    global path, target_path, product, logitem, system_info, item_smart, item_ext_smart\n",
    "\n",
    "    product = target_product\n",
    "    logitem = target_logitem\n",
    "\n",
    "    file_list = os.listdir(path)\n",
    "    file_list.sort()\n",
    "\n",
    "    # Telemetry 항목값의 Type 지정 : 필요 시 설정 (기본 None)\n",
    "    types_dict = None\n",
    "\n",
    "\n",
    "    crit_warn_info = pd.read_csv('Anomaly_Rulebase_{}.csv'.format(product), index_col=0)\n",
    "\n",
    "\n",
    "    # Histogram 용 Item_list_hist 별개 운영\n",
    "    if logitem == 'SMART':\n",
    "        if product == 'PM963':  # PM963: Temp Sensor 2까지 있음\n",
    "            item_list_hist = item_smart[1:3] + item_smart[4:19]\n",
    "        elif product in ['PM983', 'PM1725b']:  # PM983 & PM1725b: Temp Sensor 3까지 있음\n",
    "            item_list_hist = item_smart[1:3] + item_smart[4:20]\n",
    "        elif product == 'PM953':  # PM953: NVMe 1.1 spec - No. of Err Info Log Entries까지\n",
    "            item_list_hist = item_smart[1:3] + item_smart[4:15]\n",
    "    elif logitem == 'Ext_SMART':\n",
    "        if product in ['PM963', 'PM983']:\n",
    "            item_list_hist = item_ext_smart[:7]\n",
    "        elif product == 'PM953':\n",
    "            item_list_hist = [item_ext_smart[0]] + item_ext_smart[2:5]\n",
    "\n",
    "    # Histogram 위한 데이터 로드 시 가져올 columns : 필요한 column들만 사용\n",
    "    cols_hist = [system_info[0]] + [system_info[6]] + item_list_hist\n",
    "    if logitem == 'SMART':\n",
    "        cols_hist = cols_hist + [item_smart[0]]  # Critical Warning\n",
    "    elif logitem == 'Ext_SMART':\n",
    "        cols_hist = cols_hist + [item_ext_smart[7]]  # Capacitor Health\n",
    "\n",
    "\n",
    "    # Load last data (for histogram): 각 DriveSerialNumber 별 마지막 data만 남김\n",
    "    print('Loading last data file {}: '.format(file_list[-1]) + time.strftime('%Y-%m-%d %H:%M:%S', time.localtime()))\n",
    "    start = time.time()\n",
    "    last_data = pd.DataFrame()\n",
    "    # 데이터 불러오기: Histogram 작업 시 필요한 column들만 사용\n",
    "    for chunk in pd.read_csv(path + '/' + file_list[-1], usecols=cols_hist, chunksize=2000000, dtype=types_dict):\n",
    "        chunk = chunk.sort_values(by=['DriveSerialNumber', 'TIMESTAMP'])\n",
    "        chunk = chunk.drop_duplicates(subset='DriveSerialNumber', keep='last')\n",
    "        last_data = pd.concat([last_data, chunk])\n",
    "        last_data = last_data.sort_values(by=['DriveSerialNumber', 'TIMESTAMP'])\n",
    "        last_data = last_data.drop_duplicates(subset='DriveSerialNumber', keep='last')\n",
    "    end = time.time()\n",
    "    print(f'Elapsed time : {(end-start):.3f} seconds')\n",
    "\n",
    "    # (Short Term) Histogram\n",
    "    for selected_column in item_list_hist:\n",
    "        if selected_column == 'Wear_Range_Delta':  # Wear_Range_Delta: histogram 대신 bar chart 사용하여 표시\n",
    "            continue\n",
    "        hist_data = np.array(last_data[selected_column])\n",
    "        hist_data = hist_data[~np.isnan(hist_data)]\n",
    "\n",
    "        tmp = os.listdir(target_path + '/' + selected_column + '/')\n",
    "        tmp.sort()\n",
    "        tmp_info = crit_warn_info.loc[selected_column.lower()]\n",
    "\n",
    "        if tmp_info.Y_FORMAT == 'TB':\n",
    "            hist_data = hist_data * 512 * 1000 / (1024 ** 4)\n",
    "\n",
    "        bin_num = 50\n",
    "\n",
    "        # 99percentile 또는 rule 기반으로 histogram range 설정\n",
    "        value_99p = np.percentile(hist_data, 99)\n",
    "        if not np.isnan(tmp_info.Y_MAX):\n",
    "            value_99p = max(value_99p, tmp_info.Y_MAX)\n",
    "            axis_max = value_99p + (value_99p - tmp_info.Y_MIN) * 0.1\n",
    "            axis_min = max(0, tmp_info.Y_MIN - (value_99p - tmp_info.Y_MIN) * 0.1)\n",
    "        else:\n",
    "            if value_99p <= 0:  # 99percentile이 0일 때\n",
    "                value_99p = 10\n",
    "            axis_max = value_99p + (value_99p - tmp_info.Y_MIN) * 0.1\n",
    "            axis_min = max(0, tmp_info.Y_MIN - (value_99p - tmp_info.Y_MIN) * 0.1)\n",
    "\n",
    "        fig_hist, axh = plt.subplots(nrows=1, ncols=1, sharex=True, figsize=(6, 3), dpi=100)\n",
    "        axh.hist(hist_data, bins=bin_num,\n",
    "                 range=(axis_min, axis_max), color='skyblue', edgecolor='grey', linewidth=1)\n",
    "\n",
    "        axh.set_title(selected_column + ' (Histogram)', fontsize=15)\n",
    "        axh.tick_params(labelsize=11)\n",
    "\n",
    "        if tmp_info.Y_FORMAT == 'TB':\n",
    "            axh.set_xlabel(selected_column + ' (TB)', fontsize=13)\n",
    "        else:\n",
    "            axh.set_xlabel(selected_column, fontsize=13)\n",
    "\n",
    "        axh.set_ylabel('SSD Count (pcs)', fontsize=13)\n",
    "        formatter = FuncFormatter(kilo)\n",
    "        axh.yaxis.set_major_formatter(formatter)\n",
    "\n",
    "        fig_hist.tight_layout()\n",
    "        fig_hist.savefig(target_path + '/1_Figure/' + selected_column + '_Histogram.png', dpi=100)\n",
    "\n",
    "        plt.close(fig_hist)\n",
    "        fig_hist.clf()\n",
    "\n",
    "    # Bar Chart for special items (SMART: Critical Warning, Ext_SMART: Capacitor Health & Wear Range Delta)\n",
    "    if logitem == 'SMART':\n",
    "        hist_sp_items = ['CritWarning']\n",
    "    elif logitem == 'Ext_SMART':\n",
    "        hist_sp_items = ['Wear_Range_Delta', 'Capacitor_Health']\n",
    "\n",
    "    for hist_item in hist_sp_items:\n",
    "        hist_data = last_data[hist_item]\n",
    "        if hist_item == 'CritWarning':\n",
    "            width = 0.02 * 33\n",
    "            xlim = (-1, 32)\n",
    "        elif hist_item == 'Wear_Range_Delta':\n",
    "            width = 0.02 * 6\n",
    "            xlim = (-0.5, 5.5)\n",
    "        elif hist_item == 'Capacitor_Health':\n",
    "            width = 0.02 * 101\n",
    "            xlim = (-5, 105)\n",
    "\n",
    "        title = '{} (Bar Chart)'.format(hist_item)\n",
    "        xlabel = hist_item\n",
    "\n",
    "        hist_data = hist_data[~np.isnan(hist_data)]\n",
    "        value_count = hist_data.value_counts()\n",
    "\n",
    "        fig_bar, ax = plt.subplots(nrows=1, ncols=1, sharex=True, figsize=(6, 3), dpi=100)\n",
    "        ax.bar(value_count.index, value_count, color='skyblue', edgecolor='grey', width=width, linewidth=1)\n",
    "\n",
    "        ax.set_xlim(xlim)\n",
    "        ax.set_title(title, fontsize=15)\n",
    "        ax.tick_params(labelsize=11)\n",
    "\n",
    "        ax.set_xlabel(xlabel, fontsize=13)\n",
    "        ax.set_ylabel('SSD Count (pcs)', fontsize=13)\n",
    "        formatter = FuncFormatter(kilo)\n",
    "        ax.yaxis.set_major_formatter(formatter)\n",
    "\n",
    "        fig_bar.tight_layout()\n",
    "        fig_bar.savefig(target_path + '/1_Figure/{}_Barchart.png'.format(hist_item), dpi=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import os as os\n",
    "import time\n",
    "\n",
    "## MTP / pyspark 라이브러리 호출\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import types\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.neighbors import KernelDensity \n",
    "from sklearn import metrics\n",
    "#import pandas as pd #as-is\n",
    "from pyspark.sql import Row\n",
    "\n",
    "\n",
    "## MTP / Spark df => Sqlpool Table Sample\n",
    "from pyspark.sql.functions import lower, upper, col\n",
    "from pyspark.sql.functions import when\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[count(DISTINCT ITEM, CRITICAL_LOW, CRITICAL_HIGH, WARNING_LOW, WARNING_HIGH, Y_MIN, Y_MAX, Y_FORMAT): bigint]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crit_warn_info.select(count_distinct('*'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# 1.데이터 생성\n",
    "\"\"\"\n",
    "crit_warn_info = sqlContext.read.format('com.databricks.spark.csv')\\\n",
    "                .options(header='true', inferSchema='true')\\\n",
    "                .load('abfss://poc-001@stpockr001.dfs.core.windows.net/EDA/Anomaly_Rulebase_PM963.csv')\n",
    "\"\"\"\n",
    "crit_warn_info = spark.read.csv('/Users/AitheNutrigene/Desktop/Azure_pyspark/Anomaly_Rulebase_PM963.csv', header=True, inferSchema='true') #220627 kdy 추가\n",
    "item = \"critwarning\"\n",
    "#crit_warn_info.collect()\n",
    "\n",
    "# 2.SQL 서버 접속 정보 설정\n",
    "dbsrv_url = \"jdbc:sqlserver://syn-dlake-poc-krc-001.sql.azuresynapse.net:1433\"\n",
    "conProperties = {\"databasename\": \"SynSqlpool001\", \"user\": \"sqladminuser@syn-dlake-poc-krc-001\", \"password\": \"sumsung!123\"}\n",
    "save_mode = \"append\"\n",
    "\n",
    "# 3.Insert into Sqlpool\n",
    "crit_warn_info.write.jdbc(dbsrv_url, \"test_crit_warn_info\", mode=save_mode , properties=conProperties)\n",
    "\n",
    "# 4.저장한 Sqlpool에 접속하여 insert 여부 확인\n",
    "\n",
    "# 5.1.저장한 Sqlpool에서 Select\n",
    "crit_warn_info = spark.read.jdbc(dbsrv_url,\"test_crit_warn_info\",properties=conProperties)\n",
    "crit_warn_info.show(5)\n",
    "#display(crit_warn_info)\n",
    "\n",
    "# 5.2.저장한 Sqlpool에서 Distinct_Count\n",
    "dist_cnt_crit_warn_info=crit_warn_info.select(countDistinct(\"*\"))\n",
    "dist_cnt_crit_warn_info.show()\n",
    "\n",
    "# 5.3.저장한 Sqlpool에서 Count\n",
    "cnt_crit_warn_info=crit_warn_info.select(count(\"*\"))\n",
    "cnt_crit_warn_info.show()\n",
    "\n",
    "# 6.sqlContext\n",
    "crit_warn_info = crit_warn_info.registerTempTable(\"crit_warn_info\")\n",
    "crit_warn_info = sqlContext.sql(\"SELECT *\\\n",
    "                                FROM crit_warn_info where CRITICAL_LOW like 'VALUE%'\")\n",
    "crit_warn_info.show(2)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "758017f5eb1438d44a48f78d4d11fc1f68f53e5b484d3a413480cb9f8a0f95d3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
