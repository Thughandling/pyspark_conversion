{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "collect() → 해당 데이터 프레임의 모든 row 반환\n",
    "\n",
    "- [sqlContext.read](http://sqlContext.read) → DataFrame읽을때 사용  [DataFrameReader를 반환]**\n",
    "\n",
    "[sqlContext.read](http://sqlContext.read).format(’com.databricks.spark.csv’)  → csv파일을 읽어야 할때 사용 (in ver pyspark ver1.0)**\n",
    "\n",
    "\n",
    "- data.withColumn(’date’, substring(data.TIMESTAMP,1,10)) →data에 “date”라는 이름으로 오른쪽 새로운 값으로 컬럼 생성/변환\n",
    "\n",
    "⇒ df.loc[:,”date”] = substring(data.TIMESTAMP,1,10)\n",
    "\n",
    "\n",
    "- data.registerTempTable(\"df_tmp\") → data(RDD)를 “df_tmp”라는 임시 테이블로 카피(값 같음)\n",
    "\n",
    "⇒ df.copy()\n",
    "\n",
    "\n",
    "- sqlContext.sql → 데이터 프레임에 SQL 쿼리 날릴수 있음\n",
    "\n",
    "⇒ data = sqlContext.sql(\"select * from df_tmp\")\n",
    "\n",
    "\n",
    "- data.dropDuplicates(['DriveSerialNumber', 'date']) 열값을 비교하여 중복되는 데이터 **행** 제거\n",
    "\n",
    "+ keep = 'last' 옵션 → 맨마지막 값 남김\n",
    "\n",
    "\n",
    "- spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
    "\n",
    "emptyRDD = spark.sparkContext.emptyRDD()\n",
    "\n",
    "→ Creates Empty RDD\n",
    "\"\"\"\n",
    "import datetime\n",
    "import os as os\n",
    "import time\n",
    "\n",
    "## MTP / pyspark 라이브러리 호출\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import types\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "# from pyspark.sql.functions import lower, upper, col\n",
    "# from pyspark.sql.functions import when\n",
    "import numpy as np\n",
    "from sklearn.neighbors import KernelDensity \n",
    "from sklearn import metrics\n",
    "#import pandas as pd #as-is\n",
    "from pyspark.sql import Row, context\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- : array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "spark = SparkSession.builder.appName('Basics').getOrCreate()\n",
    "\n",
    "# pd.read_csv -> pyspark\n",
    "# read_pd = pd.read_csv('datasets_directory')\n",
    "# read_ps = spark.read.format('com.databricks.spark.csv').options(header='true', inferSchema='true').load('datasets_directory')\n",
    "\n",
    "\n",
    "# Empty DataFrame\n",
    "generate_empty_df =  pd.DataFrame()\n",
    "#판다스\n",
    "generate_empty_pyspark = spark.builder.appName('Basic').getOrCreate()\n",
    "generate_empty_pyspark = \"\"\"\n",
    "collect() → 해당 데이터 프레임의 모든 row 반환\n",
    "\n",
    "- [sqlContext.read](http://sqlContext.read) → DataFrame읽을때 사용  [DataFrameReader를 반환]**\n",
    "\n",
    "[sqlContext.read](http://sqlContext.read).format(’com.databricks.spark.csv’)  → csv파일을 읽어야 할때 사용 (in ver pyspark ver1.0)**\n",
    "\n",
    "\n",
    "- data.withColumn(’date’, substring(data.TIMESTAMP,1,10)) →data에 “date”라는 이름으로 오른쪽 새로운 값으로 컬럼 생성/변환\n",
    "\n",
    "⇒ df.loc[:,”date”] = substring(data.TIMESTAMP,1,10)\n",
    "\n",
    "\n",
    "- data.registerTempTable(\"df_tmp\") → data(RDD)를 “df_tmp”라는 임시 테이블로 카피(값 같음)\n",
    "\n",
    "⇒ df.copy()\n",
    "\n",
    "\n",
    "- sqlContext.sql → 데이터 프레임에 SQL 쿼리 날릴수 있음\n",
    "\n",
    "⇒ data = sqlContext.sql(\"select * from df_tmp\")\n",
    "\n",
    "\n",
    "- data.dropDuplicates(['DriveSerialNumber', 'date']) 열값을 비교하여 중복되는 데이터 **행** 제거\n",
    "\n",
    "+ keep = 'last' 옵션 → 맨마지막 값 남김\n",
    "\n",
    "\n",
    "- spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
    "\n",
    "emptyRDD = spark.sparkContext.emptyRDD()\n",
    "\n",
    "→ Creates Empty RDD\n",
    "\"\"\"\n",
    "import datetime\n",
    "import os as os\n",
    "import time\n",
    "\n",
    "## MTP / pyspark 라이브러리 호출\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import types\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "# from pyspark.sql.functions import lower, upper, col\n",
    "# from pyspark.sql.functions import when\n",
    "import numpy as np\n",
    "from sklearn.neighbors import KernelDensity \n",
    "from sklearn import metrics\n",
    "#import pandas as pd #as-is\n",
    "from pyspark.sql import Row, context\n",
    "import pandas as pd\n",
    "\n",
    "spark = SparkSession.builder.appName('SparkByExamples').getOrCreate()\n",
    "\n",
    "# pd.read_csv -> pyspark\n",
    "# read_pd = pd.read_csv('datasets_directory')\n",
    "# read_ps = spark.read.format('com.databricks.spark.csv').options(header='true', inferSchema='true').load('datasets_directory')\n",
    "\n",
    "schema = StructType([\n",
    "  StructField('', ArrayType(StringType()), True)\n",
    "#   StructField('middlename', StringType(), True),\n",
    "#   StructField('lastname', StringType(), True)a\n",
    "  ])\n",
    "\n",
    "# Empty DataFrame\n",
    "generate_empty_df =  pd.DataFrame()\n",
    "#판다스\n",
    "empty = spark.sparkContext.emptyRDD()\n",
    "# generate_empty_pyspark = spark.createDataFrame(empty)\n",
    "\n",
    "df1 = emptyRDD.toDF(schema)\n",
    "df1.printSchema()\n",
    "# generate_empty_pyspark = generate_empty_pysparkspark.createDataFrame(generate_empty_pyspark) # empty RDD -> DataFrame \n",
    "#Pyspark \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./a.txt', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.text('./a.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               value|\n",
      "+--------------------+\n",
      "|argon2-cffi @ fil...|\n",
      "|argon2-cffi-bindi...|\n",
      "|asttokens @ file:...|\n",
      "|attrs @ file:///o...|\n",
      "|backcall @ file:/...|\n",
      "|beautifulsoup4 @ ...|\n",
      "|bleach @ file:///...|\n",
      "|certifi @ file://...|\n",
      "|cffi @ file:///C:...|\n",
      "|colorama @ file:/...|\n",
      "|      cycler==0.11.0|\n",
      "|debugpy @ file://...|\n",
      "|decorator @ file:...|\n",
      "|defusedxml @ file...|\n",
      "|entrypoints @ fil...|\n",
      "|executing @ file:...|\n",
      "|fastjsonschema @ ...|\n",
      "|   fonttools==4.33.3|\n",
      "|ipykernel @ file:...|\n",
      "|ipython @ file://...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "ename": "ParseException",
     "evalue": "\nmismatched input 'INSERT' expecting {<EOF>, ';'}(line 1, pos 18)\n\n== SQL ==\n select * from df INSERT INTO b.txt\n------------------^^^\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mParseException\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\AitheNutrigene\\Desktop\\git\\0627_note.ipynb Cell 6'\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/AitheNutrigene/Desktop/git/0627_note.ipynb#ch0000015?line=0'>1</a>\u001b[0m df\u001b[39m.\u001b[39mshow()\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/AitheNutrigene/Desktop/git/0627_note.ipynb#ch0000015?line=2'>3</a>\u001b[0m spark\u001b[39m.\u001b[39;49msql(\u001b[39m\"\"\"\u001b[39;49m\u001b[39m select * from df INSERT INTO b.txt\u001b[39;49m\u001b[39m\"\"\"\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\AitheNutrigene\\anaconda3\\envs\\dacon\\lib\\site-packages\\pyspark\\sql\\session.py:723\u001b[0m, in \u001b[0;36mSparkSession.sql\u001b[1;34m(self, sqlQuery)\u001b[0m\n\u001b[0;32m    707\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msql\u001b[39m(\u001b[39mself\u001b[39m, sqlQuery):\n\u001b[0;32m    708\u001b[0m     \u001b[39m\"\"\"Returns a :class:`DataFrame` representing the result of the given query.\u001b[39;00m\n\u001b[0;32m    709\u001b[0m \n\u001b[0;32m    710\u001b[0m \u001b[39m    .. versionadded:: 2.0.0\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    721\u001b[0m \u001b[39m    [Row(f1=1, f2='row1'), Row(f1=2, f2='row2'), Row(f1=3, f2='row3')]\u001b[39;00m\n\u001b[0;32m    722\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 723\u001b[0m     \u001b[39mreturn\u001b[39;00m DataFrame(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jsparkSession\u001b[39m.\u001b[39;49msql(sqlQuery), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_wrapped)\n",
      "File \u001b[1;32mc:\\Users\\AitheNutrigene\\anaconda3\\envs\\dacon\\lib\\site-packages\\py4j\\java_gateway.py:1304\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1298\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1299\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1300\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1301\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1303\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1304\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[0;32m   1305\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[0;32m   1307\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[0;32m   1308\u001b[0m     temp_arg\u001b[39m.\u001b[39m_detach()\n",
      "File \u001b[1;32mc:\\Users\\AitheNutrigene\\anaconda3\\envs\\dacon\\lib\\site-packages\\pyspark\\sql\\utils.py:117\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    113\u001b[0m converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n\u001b[0;32m    114\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(converted, UnknownException):\n\u001b[0;32m    115\u001b[0m     \u001b[39m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[0;32m    116\u001b[0m     \u001b[39m# JVM exception message.\u001b[39;00m\n\u001b[1;32m--> 117\u001b[0m     \u001b[39mraise\u001b[39;00m converted \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m    118\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    119\u001b[0m     \u001b[39mraise\u001b[39;00m\n",
      "\u001b[1;31mParseException\u001b[0m: \nmismatched input 'INSERT' expecting {<EOF>, ';'}(line 1, pos 18)\n\n== SQL ==\n select * from df INSERT INTO b.txt\n------------------^^^\n"
     ]
    }
   ],
   "source": [
    "df.show()\n",
    "\n",
    "spark.sql(\"\"\" select * from df INSERT INTO b.txt\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import os as os\n",
    "import time\n",
    "\n",
    "## MTP / pyspark 라이브러리 호출\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import types\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "# from pyspark.sql.functions import lower, upper, col\n",
    "# from pyspark.sql.functions import when\n",
    "import numpy as np\n",
    "from pyspark.mllib.stat import KernelDensity\n",
    "from sklearn.neighbors import KernelDensity \n",
    "from sklearn import metrics\n",
    "#import pandas as pd #as-is\n",
    "from pyspark.sql import Row, context\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Primary storage account path: abfss://dl-sec@aithe.dfs.core.windows.net/\n",
      "pathList: abfss://dl-sec@aithe.dfs.core.windows.net/\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o224.load.\n: java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.azurebfs.SecureAzureBlobFileSystem not found\r\n\tat org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2595)\r\n\tat org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3269)\r\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3301)\r\n\tat org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:124)\r\n\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3352)\r\n\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3320)\r\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:479)\r\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:361)\r\n\tat org.apache.spark.sql.execution.streaming.FileStreamSink$.hasMetadata(FileStreamSink.scala:46)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:377)\r\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:325)\r\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$3(DataFrameReader.scala:307)\r\n\tat scala.Option.getOrElse(Option.scala:189)\r\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:307)\r\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:239)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.base/java.lang.Thread.run(Thread.java:834)\r\nCaused by: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.azurebfs.SecureAzureBlobFileSystem not found\r\n\tat org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2499)\r\n\tat org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2593)\r\n\t... 25 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\AitheNutrigene\\Desktop\\git\\0627_note.ipynb Cell 6'\u001b[0m in \u001b[0;36m<cell line: 606>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/AitheNutrigene/Desktop/git/0627_note.ipynb#ch0000002?line=603'>604</a>\u001b[0m pathList \u001b[39m=\u001b[39m path\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/AitheNutrigene/Desktop/git/0627_note.ipynb#ch0000002?line=604'>605</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mpathList: \u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m pathList)\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/AitheNutrigene/Desktop/git/0627_note.ipynb#ch0000002?line=605'>606</a>\u001b[0m df_pathList \u001b[39m=\u001b[39m spark\u001b[39m.\u001b[39;49mread\u001b[39m.\u001b[39;49mformat(\u001b[39m\"\u001b[39;49m\u001b[39mcsv\u001b[39;49m\u001b[39m\"\u001b[39;49m)\u001b[39m.\u001b[39;49mload(pathList)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/AitheNutrigene/Desktop/git/0627_note.ipynb#ch0000002?line=606'>607</a>\u001b[0m file_list \u001b[39m=\u001b[39m df_pathList\u001b[39m.\u001b[39minputFiles()  \n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/AitheNutrigene/Desktop/git/0627_note.ipynb#ch0000002?line=607'>608</a>\u001b[0m \u001b[39mprint\u001b[39m(file_list)\n",
      "File \u001b[1;32mc:\\Users\\AitheNutrigene\\anaconda3\\envs\\dacon\\lib\\site-packages\\pyspark\\sql\\readwriter.py:204\u001b[0m, in \u001b[0;36mDataFrameReader.load\u001b[1;34m(self, path, format, schema, **options)\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39moptions)\n\u001b[0;32m    203\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(path, \u001b[39mstr\u001b[39m):\n\u001b[1;32m--> 204\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_df(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jreader\u001b[39m.\u001b[39;49mload(path))\n\u001b[0;32m    205\u001b[0m \u001b[39melif\u001b[39;00m path \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    206\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mtype\u001b[39m(path) \u001b[39m!=\u001b[39m \u001b[39mlist\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\AitheNutrigene\\anaconda3\\envs\\dacon\\lib\\site-packages\\py4j\\java_gateway.py:1304\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1298\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1299\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1300\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1301\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1303\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1304\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[0;32m   1305\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[0;32m   1307\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[0;32m   1308\u001b[0m     temp_arg\u001b[39m.\u001b[39m_detach()\n",
      "File \u001b[1;32mc:\\Users\\AitheNutrigene\\anaconda3\\envs\\dacon\\lib\\site-packages\\pyspark\\sql\\utils.py:111\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdeco\u001b[39m(\u001b[39m*\u001b[39ma, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw):\n\u001b[0;32m    110\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 111\u001b[0m         \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39ma, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw)\n\u001b[0;32m    112\u001b[0m     \u001b[39mexcept\u001b[39;00m py4j\u001b[39m.\u001b[39mprotocol\u001b[39m.\u001b[39mPy4JJavaError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    113\u001b[0m         converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32mc:\\Users\\AitheNutrigene\\anaconda3\\envs\\dacon\\lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[39m=\u001b[39m OUTPUT_CONVERTER[\u001b[39mtype\u001b[39m](answer[\u001b[39m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[39mif\u001b[39;00m answer[\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m. Trace:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{3}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o224.load.\n: java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.azurebfs.SecureAzureBlobFileSystem not found\r\n\tat org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2595)\r\n\tat org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3269)\r\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3301)\r\n\tat org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:124)\r\n\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3352)\r\n\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3320)\r\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:479)\r\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:361)\r\n\tat org.apache.spark.sql.execution.streaming.FileStreamSink$.hasMetadata(FileStreamSink.scala:46)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:377)\r\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:325)\r\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$3(DataFrameReader.scala:307)\r\n\tat scala.Option.getOrElse(Option.scala:189)\r\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:307)\r\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:239)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.base/java.lang.Thread.run(Thread.java:834)\r\nCaused by: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.azurebfs.SecureAzureBlobFileSystem not found\r\n\tat org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2499)\r\n\tat org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2593)\r\n\t... 25 more\r\n"
     ]
    }
   ],
   "source": [
    "\n",
    "## KDEprocess_SMART_2021_v3 셀 수행\n",
    "\n",
    "# 공용 parameter\n",
    "path = None\n",
    "target_path = None\n",
    "\n",
    "\n",
    "## load_data_w_fw 함수: 파일 불러올 때 FW 정보 포함, FW: ERRORMOD 데이터 필터링\n",
    "def load_data_w_fw(file):\n",
    "    global path, target_path\n",
    "    print(file)\n",
    "\n",
    "    # 데이터 로드 시 FW 정보 관련 처리\n",
    "    cols_fw = cols + ['FirmwareRevision']\n",
    "    types_fw = {'FirmwareRevision': 'category'}\n",
    "    try:\n",
    "        types_fw.update(types_dict)\n",
    "    except TypeError:  # types_dict None일 때 pass\n",
    "        pass\n",
    "\n",
    "    print('Loading {}: '.format(file) +\n",
    "          time.strftime('%Y-%m-%d %H:%M:%S', time.localtime()))\n",
    "    start = time.time()\n",
    "    # 데이터 불러오기: KDE 작업 시 필요한 column들 + FW 정보\n",
    "\n",
    "    ## MJS / pandas에서 csv 로드 부분을 pyspark로 변경\n",
    "    # data = pd.read_csv(path + '/' + file, usecols=cols_fw, dtype=types_fw) #as-is\n",
    "    # data['date'] = data.TIMESTAMP.str[:10] #as-is\n",
    "    # # Category화를 통한 소요 메모리 절약 #as-is\n",
    "    # data['date'] = data['date'].astype('category') #as-is\n",
    "    # # Sort by: DriveSerialNumber, TIMESTAMP #as-is\n",
    "    # data = data.sort_values(by=['DriveSerialNumber', 'TIMESTAMP']) #as-is\n",
    "\n",
    "    data = SQLContext.createDataFrame()\n",
    "\n",
    "        \n",
    "    data = spark.read.format('com.databricks.spark.csv')\\\n",
    "                    .options(header='true', inferSchema='true')\\\n",
    "                    .option(\"mode\", \"DROPMALFORMED\")\\\n",
    "                    .load(file)\n",
    "    \n",
    "    # print(data.show(5))\n",
    "    data = data.withColumn('date',substring(data.TIMESTAMP,1,10)) \n",
    "    data.registerTempTable(\"df_tmp\")\n",
    "    data = sqlContext.sql(\"select * from df_tmp\")\n",
    "    \n",
    "    # Sort by: DriveSerialNumber, TIMESTAMP\n",
    "    data = data.sort('DriveSerialNumber', 'TIMESTAMP')\n",
    "\n",
    "    # FW: ERRORMOD인 데이터 별도 파일로 저장\n",
    "    ## MTP / 기존 csv에서 DataFrame로 변경\n",
    "    # df_err = data[data['FirmwareRevision'] == 'ERRORMOD'] #as-is\n",
    "    # df_err.to_csv(target_path+ '/' +'FW_ERRORMOD_{}_{}'.format(logitem, file), index=False) #as-is\n",
    "    data = sqlContext.sql(\"select * from df_tmp where FirmwareRevision = 'ERRORMOD'\")\n",
    "\n",
    "    # 불러온 데이터에서 FW: ERRORMOD 데이터 제외\n",
    "    ## MTP / 기존 csv에서 DataFrame로 변경\n",
    "    # data = data.drop(index=df_err.index) #as-is\n",
    "    data = sqlContext.sql(\"select * from df_tmp where FirmwareRevision != 'ERRORMOD'\")\n",
    "\n",
    "    # 각 날짜별 마지막 data만 남김\n",
    "     ## MTP / pandas to pyspark\n",
    "    # data = data.drop_duplicates(subset=['DriveSerialNumber', 'date'], keep='last') #as-is\n",
    "    data = data.dropDuplicates(['DriveSerialNumber', 'date']) \n",
    "    end = time.time()\n",
    "    print('Loading data {} done - Elapsed time: '.format(file) +\n",
    "          f'{(end-start):.3f} seconds')\n",
    "    return data\n",
    "\n",
    "## MTP / 변경 예정 - 시작\n",
    "## get_kde_dist 함수: 데이터에 대한 분포 추정(KDE)\n",
    "# numpy로 작업하여 dataframe으로 리턴\n",
    "def get_kde_dist(array):\n",
    "    tmp = array[~np.isnan(array)]\n",
    "    \n",
    "    # 아주 극단적인 분포에 대해 보다 나은 밀도 추정을 위해 데이터 범위 사전 조절 (0.01 ~ 99.99 percentile)\n",
    "    lower_b = np.percentile(tmp, 0.01)\n",
    "    upper_b = np.percentile(tmp, 99.99)\n",
    "    tmp = tmp[(tmp >= lower_b) & (tmp <= upper_b)]\n",
    "    tmp = tmp.reshape(-1, 1)\n",
    "    tmp_avg = np.average(tmp)\n",
    "    tmp_std = np.std(tmp)\n",
    "\n",
    "    if tmp_std != 0:\n",
    "        tmp_norm = (tmp - tmp_avg) / tmp_std\n",
    "    else:\n",
    "        tmp_norm = tmp\n",
    "\n",
    "    x = np.linspace(np.min(tmp_norm), np.max(tmp_norm), num_of_density_layer)[:, np.newaxis]\n",
    "\n",
    "    # x 값의 구간 첫 값과 마지막 값(데이터 0.01percentile / 99.99percentile)이 동일한 경우\n",
    "    # KDE 수행하는 의미가 없고 시간만 과도하게 소요되므로 해당 경우 KDE 수행하지 않도록 설정\n",
    "    if x[-1, 0] - x[0, 0] > 1e-3:  # 부동소수점 이슈 고려하여 설정\n",
    "        if data_sampling:  # Data sampling 옵션 True일 때, 1/10으로 샘플링하여 KDE 수행\n",
    "            tmp_norm_sample = tmp_norm.reshape(-1)\n",
    "            np.random.seed(0)\n",
    "            tmp_norm_sample = np.random.choice(tmp_norm_sample, len(tmp_norm_sample) // 10)\n",
    "            tmp_norm_sample = tmp_norm_sample.reshape(-1, 1)\n",
    "            kde = KernelDensity(kernel='gaussian').fit(tmp_norm_sample)\n",
    "        else:\n",
    "            kde = KernelDensity(kernel='gaussian').fit(tmp_norm)\n",
    "\n",
    "        y_pdf = np.exp(kde.score_samples(x))\n",
    "\n",
    "    else:\n",
    "        x[:, 0] = x[-1, 0]  # 부동소수점 이슈 제거\n",
    "        y_pdf = np.ones(num_of_density_layer)\n",
    "\n",
    "    # x 생성 시 자연히 크기 순이므로 sort value 불필요 (sort 시 부동소수점 이슈로 인해 index 및 cdf 계산 결과 꼬일 수 있음)\n",
    "    # as-is\n",
    "    #density = pd.DataFrame({'x': x[:, 0], 'y_pdf': y_pdf})\n",
    "    #density['y_cdf'] = [metrics.auc(density[:ind + 2].x, density[:ind + 2].y_pdf) for ind in density.index]\n",
    "    #density['y_cdf'] = density['y_cdf'] / np.max(density['y_cdf'])\n",
    "    #density['x'] = density['x'] * tmp_std + tmp_avg\n",
    "\n",
    "    # aithe - extract density['x'] and density['y_cdf'] from dataframe pyspark\n",
    "    col_x = [i for i in x[:, 0]]\n",
    "    col_y_cdf = [j for j in y_pdf]\n",
    "    density2 = [metrics.auc(col_x[:ind + 2], col_y_cdf[:ind + 2]) for ind in range(0,len(col_x))]\n",
    "    col_y_pdf = density2 / np.max(col_y_cdf)\n",
    "    col_x_new = x[:, 0] * tmp_std + tmp_avg\n",
    " \n",
    "    # as-is\n",
    "    #return density['x'], density['y_cdf']\n",
    "    # aithe - \n",
    "    return col_x_new, col_y_pdf\n",
    "\n",
    "\n",
    "# Crit list 출력\n",
    "def get_crit_list(item, df_pivot):\n",
    "    ## MTP / pandas to pyspark  \n",
    "    # tmp_info = crit_warn_info.loc[item.lower()].str.split(',') #as-is\n",
    "    tmp_info_LOW = crit_warn_info.filter(crit_warn_info.ITEM == item.lower())\n",
    "    # dataframe에서 값을 가져올때는 collect를 사용한다.\n",
    "    tmp_info_LOW = tmp_info_LOW.select(split(tmp_info_LOW.CRITICAL_LOW, ',', 2).alias('CRITICAL_LOW')).collect()\n",
    "    tmp_info_LOW = tmp_info_LOW[0]\n",
    "    \n",
    "    tmp_info_HIGH = crit_warn_info.filter(crit_warn_info.ITEM == item.lower())\n",
    "    tmp_info_HIGH = tmp_info_HIGH.select(split(tmp_info_HIGH.CRITICAL_HIGH, ',', 2).alias('CRITICAL_HIGH')).collect()\n",
    "    tmp_info_HIGH = tmp_info_HIGH[0]\n",
    "\n",
    "    spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
    "    #Creates Empty RDD\n",
    "    emptyRDD = spark.sparkContext.emptyRDD()\n",
    "\n",
    "    schema = StructType([\n",
    "        StructField(item, StringType(), True)\n",
    "        ])    \n",
    "\n",
    "    # crit_list = pd.DataFrame().index #as-is\n",
    "    # Convert empty RDD to Dataframe\n",
    "    crit_list = emptyRDD.toDF(schema)\n",
    "    #ss = SparkSession.builder.getOrCreate()\n",
    "\n",
    "    for column in df_pivot.columns:\n",
    "        ## MTP : pandas to pyspark\n",
    "        # temp = df_pivot[column].dropna() #as-is\n",
    "        temp = df_pivot.dropna(subset=column)\n",
    "        ## MTP / 변경 중, *****pandas to pyspark 변경 어려움*****\n",
    "        # as-is\n",
    "        '''\n",
    "        if (tmp_info.CRITICAL_LOW[0] == 'VALUE'):\n",
    "            crit = temp[temp < float(tmp_info.CRITICAL_LOW[1])].index\n",
    "            crit_list = crit_list.union(crit)\n",
    "        if (tmp_info.CRITICAL_HIGH[0] == 'VALUE'):\n",
    "            crit = temp[temp > float(tmp_info.CRITICAL_HIGH[1])].index\n",
    "            crit_list = crit_list.union(crit)\n",
    "        '''\n",
    "        # aithe - extract crit_list from dataframe pyspark\n",
    "        if column != 'DriveSerialNumber':\n",
    "            if (tmp_info_LOW.CRITICAL_LOW[0] == 'VALUE'):\n",
    "                # crit=temp.filter(temp[column]<float(tmp_info_LOW.CRITICAL_LOW[1])).collect()\n",
    "                crit=temp.select('DriveSerialNumber').where(temp[column]<float(tmp_info_LOW.CRITICAL_LOW[1]))\n",
    "                crit_list = crit_list.union(crit).distinct()\n",
    "\n",
    "            if (tmp_info_HIGH.CRITICAL_HIGH[0] == 'VALUE') :\n",
    "                # crit=temp.filter(temp[column]<float(tmp_info_HIGH.CRITICAL_HIGH[1])).collect()\n",
    "                crit=temp.select('DriveSerialNumber').where(temp[column]>float(tmp_info_HIGH.CRITICAL_HIGH[1]))\n",
    "                crit_list = crit_list.union(crit).distinct()\n",
    "\n",
    "    return crit_list\n",
    "    \n",
    "\n",
    "## MTP / get_crit_list 변경되면 동일하게 적용 예정\n",
    "\n",
    "## Warn list 출력\n",
    "def get_warn_list(item, df_pivot):\n",
    "    # tmp_info = crit_warn_info.loc[item.lower()].str.split(',') #as-is\n",
    "    tmp_info_LOW = crit_warn_info.filter(crit_warn_info.ITEM == item.lower())\n",
    "    tmp_info_LOW = tmp_info_LOW.select(split(tmp_info_LOW.WARNING_LOW, ',', 2).alias('WARNING_LOW')).collect()\n",
    "    tmp_info_LOW = tmp_info_LOW[0]\n",
    "    \n",
    "    tmp_info_HIGH = crit_warn_info.filter(crit_warn_info.ITEM == item.lower())\n",
    "    tmp_info_HIGH = tmp_info_HIGH.select(split(tmp_info_HIGH.WARNING_HIGH, ',', 2).alias('WARNING_HIGH')).collect()\n",
    "    tmp_info_HIGH = tmp_info_HIGH[0]\n",
    "\n",
    "    spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
    "    #Creates Empty RDD\n",
    "    emptyRDD = spark.sparkContext.emptyRDD()\n",
    "\n",
    "    schema = StructType([\n",
    "        StructField(item, StringType(), True)\n",
    "        ])    \n",
    "    \n",
    "    # crit_list = pd.DataFrame().index #as-is\n",
    "    # Convert empty RDD to Dataframe\n",
    "    crit_list = emptyRDD.toDF(schema)\n",
    "    \n",
    "    # Warning List\n",
    "    for column in df_pivot.columns:\n",
    "        #temp = df_pivot[column].dropna()\n",
    "        temp = df_pivot.dropna(subset=column)\n",
    "        # as-is\n",
    "        '''\n",
    "        if (tmp_info.WARNING_LOW[0] == 'PPM'):\n",
    "            warn = temp[temp < np.percentile(temp, float(tmp_info.WARNING_LOW[1]) / 1000000 * 100)].index\n",
    "            warn_list = warn_list.union(warn)\n",
    "        elif tmp_info.WARNING_LOW[0] == 'VALUE':\n",
    "            warn = temp[temp < float(tmp_info.WARNING_LOW[1])].index\n",
    "            warn_list = warn_list.union(warn)\n",
    "        if (tmp_info.WARNING_HIGH[0] == 'PPM'):\n",
    "            warn = temp[temp > np.percentile(temp, 100 - float(tmp_info.WARNING_HIGH[1]) / 1000000 * 100)].index\n",
    "            warn_list = warn_list.union(warn)\n",
    "        elif tmp_info.WARNING_HIGH[0] == 'VALUE':\n",
    "            warn = temp[temp > float(tmp_info.WARNING_HIGH[1])].index\n",
    "            warn_list = warn_list.union(warn)\n",
    "        '''\n",
    "        # aithe - extract warn_list from dataframe pyspark\n",
    "        if column != 'DriveSerialNumber':\n",
    "\n",
    "            if (tmp_info_LOW.WARNING_LOW[0] == 'PPM'):\n",
    "                p = float(tmp_info_LOW.WARNING_LOW[1])/ 1000000 * 100\n",
    "                warn=temp.select('DriveSerialNumber').where(temp[column]<temp.selectExpr('percentile({column},{p})'))\n",
    "                warn_list = crit_list.union(warn).distinct()\n",
    "            elif tmp_info_LOW.WARNING_LOW[0] == 'VALUE':\n",
    "                warn = temp.select('DriveSerialNumber').where(temp[column]<float(tmp_info_LOW.WARNING_LOW[1]))\n",
    "                warn_list = warn_list.union(warn)\n",
    "\n",
    "            if (tmp_info_HIGH.WARNING_HIGH[0] == 'PPM'):\n",
    "                p = 100 - float(tmp_info.WARNING_HIGH[1]) / 1000000 * 100\n",
    "                warn = temp.select('DriveSerialNumber').where(temp[column]>temp.selectExpr('percentile({column},{p})'))\n",
    "                warn_list = warn_list.union(warn)\n",
    "            elif tmp_info_HIGH.WARNING_HIGH[0] == 'VALUE':\n",
    "                warn = temp.select('DriveSerialNumber').where(temp[column]>float(tmp_info_HIGH.WARNING_HIGH[1]))\n",
    "                warn_list = warn_list.union(warn)\n",
    "\n",
    "    return warn_list\n",
    "\n",
    "## MTP / get_crit_list 변경되면 동일하게 적용 예정\n",
    "\n",
    "## Warn list 및 boundary 출력\n",
    "def get_warn_list_boundary(item, df_pivot):\n",
    "    \n",
    "    # tmp_info = crit_warn_info.loc[item.lower()].str.split(',')\n",
    "    tmp_info_LOW = crit_warn_info.filter(crit_warn_info.ITEM == item.lower())\n",
    "    tmp_info_LOW = tmp_info_LOW.select(split(tmp_info_LOW.WARNING_LOW, ',', 2).alias('WARNING_LOW')).collect()\n",
    "    tmp_info_LOW = tmp_info_LOW[0]\n",
    "    \n",
    "    tmp_info_HIGH = crit_warn_info.filter(crit_warn_info.ITEM == item.lower())\n",
    "    tmp_info_HIGH = tmp_info_HIGH.select(split(tmp_info_HIGH.WARNING_HIGH, ',', 2).alias('WARNING_HIGH')).collect()\n",
    "    tmp_info_HIGH = tmp_info_HIGH[0]\n",
    "    \n",
    "    spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
    "    \n",
    "    #Creates Empty RDD\n",
    "    emptyRDD = spark.sparkContext.emptyRDD()\n",
    "\n",
    "    schema = StructType([\n",
    "        StructField(item, StringType(), True)\n",
    "        ])    \n",
    "    \n",
    "    # crit_list = pd.DataFrame().index #as-is\n",
    "    # Convert empty RDD to Dataframe\n",
    "    warn_list = emptyRDD.toDF(schema)\n",
    "\n",
    "    # aithe - create three emty lists for \n",
    "    lower_list = []\n",
    "    upper_list = []\n",
    "    row_lower_upper=[]\n",
    "    \n",
    "    # Warning List for upper_list, upper_list\n",
    "    for column in df_pivot.columns:\n",
    "        # temp = df_pivot[column].dropna()\n",
    "        temp = df_pivot.dropna(subset=column)\n",
    "\n",
    "        # as is \n",
    "        '''\n",
    "        if (tmp_info.WARNING_LOW[0] == 'PPM'):\n",
    "            lower_b = np.percentile(temp, float(tmp_info.WARNING_LOW[1]) / 1000000 * 100)\n",
    "            lower_list.append(lower_b)\n",
    "            warn = temp[temp < lower_b].index\n",
    "            warn_list = warn_list.union(warn)\n",
    "        elif tmp_info.WARNING_LOW[0] == 'VALUE':\n",
    "            lower_b = float(tmp_info.WARNING_LOW[1])\n",
    "            lower_list.append(lower_b)\n",
    "            warn = temp[temp < lower_b].index\n",
    "            warn_list = warn_list.union(warn)\n",
    "        else:\n",
    "            lower_list.append(np.nan)\n",
    "\n",
    "        if (tmp_info.WARNING_HIGH[0] == 'PPM'):\n",
    "            upper_b = np.percentile(temp, 100 - float(tmp_info.WARNING_HIGH[1]) / 1000000 * 100)\n",
    "            upper_list.append(upper_b)\n",
    "            warn = temp[temp > upper_b].index\n",
    "            warn_list = warn_list.union(warn)\n",
    "        elif tmp_info.WARNING_HIGH[0] == 'VALUE':\n",
    "            upper_b = float(tmp_info.WARNING_HIGH[1])\n",
    "            upper_list.append(upper_b)\n",
    "            warn = temp[temp > upper_b].index\n",
    "            warn_list = warn_list.union(warn)\n",
    "        else:\n",
    "            upper_list.append(np.nan)\n",
    "        '''\n",
    "        # aithe - extract upper_list, upper_list and warn_list from dataframe pyspark\n",
    "        if column != 'DriveSerialNumber':\n",
    "            tmp_set=['PPM','VALUE']\n",
    "            if (tmp_info_LOW.WARNING_LOW[0] in tmp_set):\n",
    "                if (tmp_info_LOW.WARNING_LOW[0] == 'PPM'):\n",
    "                    temp1= temp.select(temp.columns[1]).collect()\n",
    "   \n",
    "                    output_temp1=[i[0] for i in temp1]\n",
    "                    output_temp1=np.array(output_temp1, dtype=float)\n",
    "                    p = float(tmp_info_LOW.WARNING_LOW[1])/ 1000000 * 100\n",
    "                    \n",
    "                    lower_b = np.percentile(output_temp1, p)\n",
    "                    lower_list.append(lower_b)\n",
    "                    # warn = temp[temp < lower_b].index\n",
    "                    warn=temp.select('DriveSerialNumber').where(temp[column]<lower_b)\n",
    "                    warn_list = warn_list.union(warn)\n",
    "                elif tmp_info_LOW.WARNING_LOW[0] == 'VALUE':\n",
    "                    lower_b = float(tmp_info_LOW.WARNING_LOW[1])\n",
    "                    lower_list.append(lower_b)\n",
    "                    # warn = temp[temp < lower_b].index\n",
    "                    warn=temp.select('DriveSerialNumber').where(temp[column]<lower_b)\n",
    "                    warn_list = warn_list.union(warn)\n",
    "            else:\n",
    "                lower_list.append(np.nan)\n",
    "\n",
    "            if(tmp_info_HIGH.WARNING_HIGH[0] in tmp_set):\n",
    "                if (tmp_info_HIGH.WARNING_HIGH[0] == 'PPM'):\n",
    "                    temp1= temp.select(temp.columns[1]).collect()\n",
    "   \n",
    "                    output_temp1=[i[0] for i in temp1]\n",
    "                    output_temp1=np.array(output_temp1, dtype=float)\n",
    "                    p = 100 - float(tmp_info_HIGH.WARNING_HIGH[1]) / 1000000 * 100\n",
    "\n",
    "                    upper_b = np.percentile(output_temp1, p)\n",
    "                    # upper_b = temp.selectExpr('percentile({column},{p})') #aithe as is\n",
    "\n",
    "                    upper_list.append(upper_b)\n",
    "                    # warn = temp[temp > upper_b].index\n",
    "                    warn=temp.select('DriveSerialNumber').where(temp[column]> upper_b)\n",
    "                    warn_list = warn_list.union(warn)\n",
    "                elif tmp_info_HIGH.WARNING_HIGH[0] == 'VALUE':\n",
    "                    upper_b = float(tmp_info_HIGH.WARNING_HIGH[1])\n",
    "                    upper_list.append(upper_b)\n",
    "                    # warn = temp[temp > upper_b].index\n",
    "                    warn=temp.select('DriveSerialNumber').where(temp[column]> upper_b)\n",
    "                    warn_list = warn_list.union(warn)\n",
    "            else:\n",
    "                upper_list.append(np.nan)\n",
    "            \n",
    "            \n",
    "    # as-is\n",
    "    #warn_boundary = pd.DataFrame({'Lower': lower_list, 'Upper': upper_list})\n",
    "\n",
    "    # aithe - create a new dataframe for warn_boundary that merges lower_list and upper_list\n",
    "    spark = SparkSession.builder.appName('sparkdf').getOrCreate()\n",
    "    for j in range(len(lower_list)):\n",
    "        row_lower_upper.append((float(lower_list[j]), float(upper_list[j])))\n",
    "\n",
    "    columns=[\"Lower\", \"Upper\"]      \n",
    "    warn_boundary= spark.createDataFrame(row_lower_upper, columns)\n",
    "\n",
    "    return warn_list, warn_boundary\n",
    "\n",
    "\n",
    "# process_selected_column 함수:\n",
    "# 선택한 column에 대해 데이터 처리 작업\n",
    "# (Critical / Warning SN list 및 데이터 분포 정보 csv 파일로 저장)\n",
    "# 2021.09.08 수정: KDE 미수행 옵션 추가\n",
    "def process_selected_column(selected_column, data, kde=True):\n",
    "    global path, target_path\n",
    "    print('{}: '.format(selected_column) +\n",
    "          time.strftime('%Y-%m-%d %H:%M:%S', time.localtime()))\n",
    "\n",
    "    # Pivot : DriveSerialNumber x Date\n",
    "    sub_df = data[['DriveSerialNumber', 'date', selected_column]]\n",
    "    \n",
    "    ## MTP / pandas to pyspark\n",
    "    # as-is\n",
    "    # sub_df_pivot = sub_df.pivot(index='DriveSerialNumber', columns='date', values=selected_column) #as-is\n",
    "    \n",
    "    # aithe - modifying sub_df_pivot\n",
    "    sub_df_pivot = sub_df.groupBy(\"DriveSerialNumber\").pivot(\"date\").avg(selected_column)\n",
    "\n",
    "    \n",
    "    crit_list = get_crit_list(selected_column, sub_df_pivot)  # Critical List\n",
    "\n",
    "    # ## MTP / 수정 예정\n",
    "    # as-is\n",
    "    # sub_df_pivot = sub_df_pivot.loc[~sub_df_pivot.index.isin(crit_list)]  # Critical 포함 SN 제외\n",
    "\n",
    "    # aithe - modify sub_df_pivot\n",
    "    crit_list1= crit_list.select(crit_list.columns[0]).collect()\n",
    "    output_crit_list=[i[0] for i in crit_list1]\n",
    "    sub_df_pivot=sub_df_pivot.filter(~col('DriveSerialNumber').isin(output_crit_list))\n",
    "\n",
    "    \n",
    "    warn_list, warn_boundary = get_warn_list_boundary(selected_column, sub_df_pivot)  # Warning List & Boundary\n",
    "  \n",
    "    # aithe - converts warn_boundary to list\n",
    "    warn_list1= warn_list.select(warn_list.columns[0]).collect()\n",
    "    output_warn_list=[i[0] for i in warn_list1]\n",
    "\n",
    "\n",
    "    s = os.path.split(file)\n",
    "\n",
    "    # ## MTP / csv 저장 대신 Sqlpool 사용으로 주석처리함\n",
    "    # Data Save #as-is\n",
    "    # if os.path.exists(target_path +'/' + selected_column) == False: #as-is\n",
    "    #     os.mkdir(target_path +'/' + selected_column) #as-is\n",
    "\n",
    "    # # pd.DataFrame(crit_list).to_csv(target_path +'/' + selected_column + '/Crit_' + file) #as-is\n",
    "    # # pd.DataFrame(warn_list).to_csv(target_path +'/'+ selected_column + '/Warn_' + file) #as-is\n",
    "    # # warn_boundary.to_csv(target_path +'/' + selected_column + '/Boundary_' + file) #as-is\n",
    "    #df_crit_list = spark.createDataFrame(crit_list, StringType())\n",
    "    crit_list.coalesce(1).write.format(\"com.databricks.spark.csv\").option(\"header\", \"true\").mode('overwrite').save(target_path +'/' + selected_column + '/Crit_' + s[1])\n",
    "    warn_list.coalesce(1).write.format(\"com.databricks.spark.csv\").option(\"header\", \"true\").mode('overwrite').save(target_path +'/' + selected_column + '/Warn_' + s[1])\n",
    "    warn_boundary.coalesce(1).write.format(\"com.databricks.spark.csv\").option(\"header\", \"true\").mode('overwrite').save(target_path +'/' + selected_column + '/Boundary_' + s[1])\n",
    "    \n",
    "    # ## MTP / 변경 예정\n",
    "    if kde == True:\n",
    "        # as-is\n",
    "        # sub_df_pivot = sub_df_pivot.loc[~sub_df_pivot.index.isin(warn_list)]  # Warning 포함 SN 제외\n",
    "\n",
    "        # aithe - modify sub_df_pivot to extract ary_normal\n",
    "        sub_df_pivot1=sub_df_pivot.filter(~col('DriveSerialNumber').isin(output_warn_list))\n",
    "       \n",
    "        sub_df_pivot2=sub_df_pivot1.select(sub_df_pivot1.columns[1:len(sub_df_pivot1.columns)]).collect()\n",
    "        ary_normal = np.array(sub_df_pivot2, dtype=float)\n",
    "        \n",
    "    #     # Density Estimation : KDE\n",
    "        day = ary_normal.shape[1]\n",
    "        SDist = np.zeros((day, num_of_density_layer))  ### Day by Day KDE Based CDF\n",
    "        weight = np.zeros((day, num_of_density_layer))  ### Day by Day Weight (CDF Diff)\n",
    "        for i in range(day):\n",
    "            SDist[i,], weight[i,] = get_kde_dist(ary_normal[:, i])\n",
    "    \n",
    "    # as-is\n",
    "    #     pd.DataFrame(SDist).to_csv(target_path +'/'+ selected_column + '/Value_' + file)\n",
    "    #     pd.DataFrame(weight).to_csv(target_path +'/' + selected_column + '/Weight_' + file)\n",
    "        # numpy를 Dataframe로 변환\n",
    "        s2=[]\n",
    "        if len(weight)>=0:\n",
    "            for i in weight:\n",
    "                s1=[j.item() for j in i]\n",
    "                s2.append(s1)\n",
    "\n",
    "        dfweight = spark.createDataFrame(s2)\n",
    "        # 단일 CSV 파일로 저장\n",
    "        dfweight.coalesce(1).write.format(\"com.databricks.spark.csv\").option(\"header\", \"true\").mode('overwrite').save(target_path +'/' + selected_column + '/Weight_' + s[1])\n",
    "\n",
    "        s2=[]\n",
    "        # numpy를 Dataframe로 변환\n",
    "        if len(SDist)>=0:\n",
    "            for i in SDist:\n",
    "                s1=[j.item() for j in i]\n",
    "                s2.append(s1)\n",
    "        dfSDist = spark.createDataFrame(s2)\n",
    "        # 단일 CSV 파일로 저장\n",
    "        dfSDist.coalesce(1).write.format(\"com.databricks.spark.csv\").option(\"header\", \"true\").mode('overwrite').save(target_path +'/' + selected_column + '/Value_' + s[1])\n",
    "        \n",
    "\n",
    "\n",
    "def KDE_main(target_product, target_logitems, num_of_last_week):\n",
    "    global path, target_path, product, logitem, system_info, cols, types_dict, crit_warn_info, file_list, data_sampling, num_of_density_layer, file\n",
    "\n",
    "    product = target_product\n",
    "    logitem = target_logitems\n",
    "\n",
    "    print('Start making data for KDE')\n",
    "\n",
    "    # System 정보\n",
    "    system_info = ['TIMESTAMP', 'Cluster', 'NodeId', 'Generation', 'HwSkuId',\n",
    "                   'DriveProductId', 'DriveSerialNumber', 'FirmwareRevision']\n",
    "\n",
    "    # SMART 항목\n",
    "    item_smart = ['CritWarning', 'Temperature', 'AvailableSpare', 'AvailSpareThreshold',\n",
    "                  'PercentageUsed', 'DataUnitsRead', 'DataUnitsWritten',\n",
    "                  'HostReadCommands', 'HostWriteCommands', 'ControllerBusyTime',\n",
    "                  'PowerCycles', 'PowerOnHours', 'UnsafeShutdowns', 'MediaErrors',\n",
    "                  'NumErrInfoLogEntries', 'WarnCompositeTempTime',\n",
    "                  'CritCompositeTempTime', 'TempSensor1', 'TempSensor2', 'TempSensor3',\n",
    "                  'TempSensor4', 'TempSensor5', 'TempSensor6', 'TempSensor7',\n",
    "                  'TempSensor8']\n",
    "\n",
    "    # Extended SMART 항목 전부 표기 - KDE 미수행 항목들은 뒤쪽에 표기\n",
    "    item_ext_smart = ['Media_Units_Written', 'ECC_Iterations', 'Wear_Range_Delta',\n",
    "                      'Unaligned_IO', 'Mapped_LBAs', 'Program_Fail_Count', 'Erase_Fail_Count',\n",
    "                      'Capacitor_Health', 'Supported_Features', 'Power_Consumption',\n",
    "                      'Temperature_Throttling']\n",
    "\n",
    "    ## 제품 / Log item 별 item list 설정\n",
    "    # 2021.09.08 수정: Critical Warning과 Capacitor Health도 Crit/Warn list 추출 및 KDE 작업 수행\n",
    "    if logitem == 'SMART':\n",
    "        if product == 'PM963':  # PM963: Temp Sensor 2까지 있음\n",
    "            item_list = item_smart[0:3] + item_smart[4:19]\n",
    "        elif product in ['PM983', 'PM1725b']:  # PM983 & PM1725b: Temp Sensor 3까지 있음\n",
    "            item_list = item_smart[0:3] + item_smart[4:20]\n",
    "        elif product == 'PM953':  # PM953: NVMe 1.1 spec - No. of Err Info Log Entries까지\n",
    "            item_list = item_smart[0:3] + item_smart[4:15]\n",
    "    elif logitem == 'Ext_SMART':\n",
    "        if product in ['PM963', 'PM983']:\n",
    "            item_list = item_ext_smart[:8]\n",
    "        elif product == 'PM953':  # PM953: Ext SMART 지원 항목 적음\n",
    "            item_list = [item_ext_smart[0]] + item_ext_smart[2:5]\n",
    "\n",
    "    # 데이터 로드 시 가져올 columns : 필요한 column들만 사용\n",
    "    cols = [system_info[0]] + [system_info[6]] + item_list\n",
    "\n",
    "    # Telemetry 항목값의 Type 지정 : 필요 시 설정 (기본 None)\n",
    "    types_dict = None\n",
    "\n",
    "    ## MTP / pandas to pyspark\n",
    "    # 제품별 Critical / Warning 정보 불러오기\n",
    "    # crit_warn_info = pd.read_csv('Anomaly_Rulebase_{}.csv'.format(product), index_col=0) #as-is\n",
    "    crit_warn_info = sqlContext.read.format('com.databricks.spark.csv')\\\n",
    "                .options(header='true', inferSchema='true')\\\n",
    "                .load(adls_path + '/datasets/Anomaly_Rulebase_{}.csv'.format(product))\n",
    "\n",
    "    #aithe - add data type Y_MAX from int -> float\n",
    "    crit_warn_info = crit_warn_info.withColumn(\"Y_MAX\", crit_warn_info[\"Y_MAX\"].cast('float'))\n",
    "\n",
    "    \n",
    "    ## MTP / pandas to pyspark시 os 함수 호출 불가, make_EDA_plot에서 file_list 호출함\n",
    "    # # 파일 경로 및 파일 목록 #as-is\n",
    "    # print('file path :', path)\n",
    "    # file_list = os.listdir(path)\n",
    "    # file_list.sort()\n",
    "    \n",
    "    # 동작 시간 단축을 위해 data를 sampling하여 사용할 지 옵션\n",
    "    data_sampling = True\n",
    "\n",
    "    num_of_density_layer =  100\n",
    "\n",
    "    ### Density Estimation - All\n",
    "\n",
    "    # file_list 범위 설정\n",
    "    file_list = file_list[-num_of_last_week:]\n",
    "    print(file_list)\n",
    "    print('above_files')\n",
    "\n",
    "    for file in file_list:\n",
    "        print('Processing {}: '.format(file) +\n",
    "              time.strftime('%Y-%m-%d %H:%M:%S', time.localtime()))\n",
    "        start = time.time()\n",
    "        data = load_data_w_fw(file)  # FW ERRORMOD 필터링 적용\n",
    "\n",
    "        for selected_column in item_list:\n",
    "            process_selected_column(selected_column, data)\n",
    "\n",
    "        del data  # For memory\n",
    "\n",
    "        end = time.time()\n",
    "        print('Processing {} done - Elapsed time: '.format(file) +\n",
    "              f'{(end-start):.3f} seconds')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########################### main conduct part#################################\n",
    "## make_EDA_plot 셀 수행\n",
    "\n",
    "## MTP / 노트북 통합으로 인한 주석\n",
    "# import KDEprocess_SMART_2021_v3 as kde_f\n",
    "# import Visualization_SMART_2021_v3 as viz_f\n",
    "\n",
    "#공용 parameter\n",
    "num_of_last_week = 2 #새로 KDE 추출할 데이터 갯수(Week 단위 기준)\n",
    "\n",
    "## MTP / adls 경로 추출\n",
    "# Primary storage info\n",
    "account_name = 'aithe' # fill in your primary account name\n",
    "container_name = 'dl-sec' # fill in your container name\n",
    "#relative_path = 'EDA/Telemetry_raw/Daily/To_Weekly/PM963/SMART/' # fill in your relative folder path\n",
    "relative_path = 'EDA/' # fill in your relative folder path#\n",
    "\n",
    "adls_path = 'abfss://%s@%s.dfs.core.windows.net/'% (container_name, account_name)\n",
    "print('Primary storage account path: ' + adls_path)\n",
    "\n",
    "####### PM963\n",
    "\n",
    "## Parameters 입력 : 제품 / Log 아이템\n",
    "product = 'PM963' # PM963, PM983, PM1725b, PM953\n",
    "\n",
    "#읽어올 파일 경로 및 파일 목록\n",
    "#path = 'F:/Telemetry_raw/Daily/To_Weekly/{}/SMART'.format(product) #as-is\n",
    "\n",
    "## MTP / 신규 추가\n",
    "# path = adls_path + 'Telemetry_raw/Daily/To_Weekly/{}/SMARTAN'.format(product)\n",
    "path = adls_path\n",
    "# pathList = path + \"*\"\n",
    "pathList = path\n",
    "print('pathList: ' + pathList)\n",
    "df_pathList = spark.read.format(\"csv\").load(pathList)\n",
    "file_list = df_pathList.inputFiles()  \n",
    "print(file_list)\n",
    "# files = df_pathList.inputFiles()    \n",
    "# file_list = files.sort(reverse=True)\n",
    "\n",
    "#KDE 시각화를 위해 생성되는 파일들 저장 위치\n",
    "#target_path = 'F:/Telemetry_output/MSC/{}'.format(product) #as-is\n",
    "\n",
    "## MTP / 변경\n",
    "#target_path = adls_path + 'Telemetry_output/MSC/{}'.format(product)\n",
    "target_path = adls_path+'MSC/{}'.format(product)\n",
    "print('path : ' + path)\n",
    "print('target_path : ' + target_path)\n",
    "\n",
    "#file_list = os.listdir(path).sort() #as-is\n",
    "### Make Data for KDE estimation \n",
    "#kde_f.path = path #SMART 데이터 위치 #as-is\n",
    "#kde_f.target_path = target_path #KDE용 생성 데이터 저장 위치 #as-is\n",
    "\n",
    "## MTP / 노트북 통합으로 kde_f 제거\n",
    "# kde_f.KDE_main(product,'SMART', num_of_last_week) #as-is\n",
    "# kde_f.KDE_main(product,'Ext_SMART', num_of_last_week) #as-is\n",
    "KDE_main(product,'SMART', num_of_last_week)\n",
    "# KDE_main(product,'Ext_SMART', num_of_last_week)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('dacon')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ff1171e120976ca7c53c41cd6e3105d8d93f11f1c83a305037e68a8c22b7d457"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
